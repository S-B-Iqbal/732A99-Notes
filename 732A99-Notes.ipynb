{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\r\n",
    "\r\n",
    "[1. Data-Manipulation](#1-Preprocessing)    \r\n",
    "|   [1.1 Load Excel Data](#1-load-excel)    \r\n",
    "|   [1.2 Denormalize normalized data](#1-denormalize)   \r\n",
    "|   [1.3 Train/Test Split](#1-train-test)   \r\n",
    "|   [1.4 Train/Test/Validation Split](#1-t-t-v)   \r\n",
    "|   [1.4 Dichotomization](#1-binarize)    \r\n",
    "|   [1.5 Writing Custom formula](#1-formula)      \r\n",
    "|   [1.6 Gaussian Kernel](#1-gaussian)    \r\n",
    "|   [1.7 Package Vignette](#1-vignette)   \r\n",
    "|   [1.8 Metrics](#1-metrics)   \r\n",
    "|   [1.9 Plotting Help](#1-plot)      \r\n",
    "\r\n",
    "----\r\n",
    "## Supervised Learning\r\n",
    "----\r\n",
    "\r\n",
    "[Neural-Net Regression](#2-nn-r)        \r\n",
    "[EM : Multivariate Bernoulli](#2-em-b)    \r\n",
    "[EM: Mixture of Gaussians](#2-em-g)   \r\n",
    "[GAM(Generalized Additive Models)](2-gam)     \r\n",
    "[Kernel Classification(using Kernel Density Estimation)](#2-kde)    \r\n",
    "[LDA(Manual implementation can be extended to QDA and Naive Bayes)](#2-lda)   \r\n",
    "[QDA](#2-qda)     \r\n",
    "[Decision Trees: Classification/Regression](#2-dt)    \r\n",
    "[Splines: Using base R Functions only](#2-splines)    \r\n",
    "[Splines: Using splines library](#2-splines2)   \r\n",
    "[Logistic Regression](#2-logistic)     \r\n",
    "[K Nearest Neighbors](#2-knn)  \r\n",
    "[Ridge Regression](#2-ridge)    \r\n",
    "[Poisson Regression](#2-poisson)    \r\n",
    "[Ridge Regression using Optim](#2-ridge-optim)       \r\n",
    "[LASSO](#2-lasso)   \r\n",
    "[Elastic-Net](#2-elastic)   \r\n",
    "[SVM](#2-svm)     \r\n",
    "[Experimental Estimation of Bagging Error](#2-bag)    \r\n",
    "[NN from Scratch](#2-nn-scratch)    \r\n",
    "\r\n",
    "----\r\n",
    "## Unsupervised Learning\r\n",
    "----\r\n",
    "[Nearest Shrunken Centroid](#ncs)   \r\n",
    "[PCA](#pca) \r\n",
    "\r\n",
    "----\r\n",
    "## Others\r\n",
    "----\r\n",
    "[Non-Parametric Bootstrap](#npb)  \r\n",
    "[Parametric Bootstrap(Regression)](#pb-r)   \r\n",
    "[Parametric Bootstrap(Classification)](#pb-c)     \r\n",
    "[Bonferroni Method](#bonferroni)    \r\n",
    "[Benjamin-Hochberg](#b-h)     \r\n",
    "[N-Fold CV](#n-cv)      \r\n",
    "[Variable Selection using StepAIC](#step-aic)       \r\n",
    "[Nested Cross-Validation](#nested-cv)   \r\n",
    "[Laplace Prior](#laplace)   \r\n",
    "[Entropy](#entropy)     \r\n",
    "\r\n",
    "\r\n",
    "<h2 id=\"1-Preprocessing\">Data-Manipulation</h2>\r\n",
    "\r\n",
    "<h3 id=\"1-load-excel\">1.1 Load Excel Data</h3>\r\n",
    "   \r\n",
    "```r\r\n",
    "library(\"readxl\")\r\n",
    "data = read_excel('spambase.xlsx')\r\n",
    "data = read_excel('spambase.xls')\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"1-denormalize\">1.2 Denormalize normalized data</h3>  \r\n",
    "\r\n",
    "  - Collect $\\textit{max}_x$, $\\textit{min}_x$  \r\n",
    "  - Apply the formula on all $\\textit{x}$ : $\\textit{x}$ * ( $\\textit{max}_x$ - $\\textit{min}_x$ ) + $\\textit{min}_x$\r\n",
    "\r\n",
    "\r\n",
    "<h3 id=\"1-train-test\">1.3 Train/Test Split</h3>   \r\n",
    "\r\n",
    "```r\r\n",
    "n = dim(data)[1]\r\n",
    "set.seed(12345)\r\n",
    "id = sample(1:n, floor(n*0.5))\r\n",
    "train = data[id,]\r\n",
    "test = data[-id,]\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"1-t-t-v\">1.3 Train/Test/Validation Split</h3>  \r\n",
    "\r\n",
    "  - Also Available in Lecture: Block 1(e)]  \r\n",
    "  - Contains example on how to perform Cross-Validation given that there is a function `loss` to estimate the loss.  \r\n",
    "\r\n",
    "```r\r\n",
    "spec = c(train = .6, test = .2, validate = .2)\r\n",
    "g = sample(cut(\r\n",
    "  seq(nrow(dataframe)), \r\n",
    "  nrow(dataframe)*cumsum(c(0,spec)),\r\n",
    "  labels = names(spec)\r\n",
    "))\r\n",
    "res = split(dataframe, g)\r\n",
    "train = res$train\r\n",
    "test = res$test\r\n",
    "val = res$validate\r\n",
    "# Cross Validation\r\n",
    "n_fold_cv = function(X,Y, n_fold){\r\n",
    "  #Randomly shuffle the data\r\n",
    "  ran_ind = sample(nrow(X))\r\n",
    "  X = X[ran_ind,]\r\n",
    "  Y = Y[ran_ind]\r\n",
    "  #Create n equally size folds\r\n",
    "  folds <- cut(seq(1,nrow(X)),breaks=n_fold,labels=FALSE)\r\n",
    "  #Perform 10 fold cross validation\r\n",
    "  cv_loss = c()\r\n",
    "  for(i in 1:n_fold){\r\n",
    "    #Segement your data by fold using the which() function \r\n",
    "    testIndexes <- which(folds==i,arr.ind=TRUE)\r\n",
    "    xTest <- X[testIndexes,]\r\n",
    "    yTest <- Y[testIndexes]\r\n",
    "    xTrain <- X[-testIndexes,]\r\n",
    "    yTrain <- Y[-testIndexes]\r\n",
    "    l = loss(x=xTrain, y = yTrain,xTest = xTest, yTest = yTest)\r\n",
    "    cv_loss = c(l,cv_loss)\r\n",
    "  }\r\n",
    "  cv_loss = mean(cv_loss)\r\n",
    "  return(cv_loss)\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"1-binarize\">1.4 Dichotomization</h3> \r\n",
    "\r\n",
    "```r\r\n",
    "spam$target = ifelse(spam$Spam==1, 'S','H') # Create a binary variable\r\n",
    "spam$target = as.factor(spam$target) # Convert to Factor\r\n",
    "contrasts(spam$target) # Check the factor\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"1-formula\">1.5 Writing Custom formula</h3> \r\n",
    "\r\n",
    "```r\r\n",
    "covariates = names(spam)[0:48] # Collect Covariates 1st\r\n",
    "cov=paste(covariates, collapse='+') # Collapse the covariates\r\n",
    "form = as.formula(paste0('target~',cov)) # Create the formula with the target\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"1-gaussian\">1.6 Gaussian Kernel</h3> \r\n",
    "\r\n",
    "  -  `h` specifies Kernel Width \r\n",
    "\r\n",
    "```r\r\n",
    "gaussian_k <- function(x, h) { return (exp(-(x**2)/(2*h*h))) }\r\n",
    "```\r\n",
    "<h3 id=\"1-vignette\">1.7 Package Vignette</h3> \r\n",
    "\r\n",
    "```r\r\n",
    "utils::browseVignettes(\"gbm\") # Package Name\r\n",
    "```\r\n",
    "<h3 id=\"1-metrics\">1.8 Metrics</h3>   \r\n",
    "\r\n",
    "  - Look at data imbalance when quoting misclassification.    \r\n",
    "- Misclassification rate : $\\textbf{1 - Accuracy    OR  }\\frac{\\text{FP+FN}}{total}$ |  Accuracy : $\\frac{TP+TN}{total}$    \r\n",
    "\r\n",
    "```r\r\n",
    "1 - mean(data$train == data$prediction)\r\n",
    "```\r\n",
    "\r\n",
    "- | Sensitivity/Recal/True Positive Rate: $\\frac{TP}{\\text{Actual Yes}}$\r\n",
    "- |       Specificity/False Positive Rate: $\\frac{\\text{FP}}{\\textbf{Actual NO}}$   \r\n",
    "```r\r\n",
    "get.rate = function(predicted.prob, actual, true_label, false_label, threshold){\r\n",
    "  prediction = ifelse(predicted.prob>threshold, true_label,false_label)\r\n",
    "  TPR = sum(prediction==true_label & actual == true_label)/sum(actual == true_label)\r\n",
    "  FPR = sum(prediction==true_label & actual == false_label)/ sum(actual == false_label)\r\n",
    "  return(c(TPR, FPR))\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "- AIC/BIC/$C_p$:\r\n",
    "    - Can be used for estimating only 'In-Sample' errors for 'Linear' Parameters.    \r\n",
    "    'In-Sample' is not beneficial as it does not represent generalized error i.e., how the model will perform on 'Unseen' data.\r\n",
    "    - AIC Formula: $\\frac{-2}{N}\\cdot\\text{log-likelihood}+ 2\\cdot\\frac{degree of freedom}{N}$ where $\\text{N = No of datapoints}$\r\n",
    "    - BIC Formula: $\\textit{Log-Likelihood} - \\frac{M}{2} \\cdot \\ln(N)$ where $M$ is the no. of parameters. $N$ = # of datapoints.\r\n",
    "        - `BIC = function(LL,M,N){return(LL - (0.5*M/log(N)))}`\r\n",
    "- Loss Matrix: \r\n",
    "    - Make sure the loss matrix and prediction order is same. Ex: if loss is `c(\"cat\",\"dog\")` then predicted probability columns should also be: `pred_prob[,c(\"cat\",\"dog\")]`\r\n",
    "    - Then use `apply(pred_prob%*%loss_matrix,MARGIN=1,FUN=which.max)`. Use the index same as `c(\"cat\",\"dog\")` for classification.\r\n",
    "\r\n",
    "<h3 id=\"1-plot\">1.9 Plotting Help</h3>  \r\n",
    "\r\n",
    "refer: https://bookdown.org/rdpeng/exdata/the-base-plotting-system-1.html#base-plotting-functions\r\n",
    "  \r\n",
    "\r\n",
    "<h2 id=\"2-head\">2. Supervised Learning</h2>   \r\n",
    "----- \r\n",
    "\r\n",
    "<h3 id=\"2-nn-r\">Neural-Net Regression</h3>  \r\n",
    "\r\n",
    "  - **STEP-1**: Standardize the Data. If Test set is unseen, then use 'mean' and 'variance' of training data.\r\n",
    "  - `y~.` does not work, formula needs to be specified specifically.\r\n",
    "  - `linear.output=TRUE` should be chosen.\r\n",
    "  - `threshold` specifies the change in Error(given by `nn.fit$err.fct`) over the epochs(also referred as 'Steps')\r\n",
    "  - Custom activation function should go into: `act.fct`:    \r\n",
    "    ```r\r\n",
    "    RelU = function(x){ ifelse(x>0,x,0)} # RELU Implementation\r\n",
    "    nn.fit = neuralnet(form, data = tr, hidden = c(2,2,2), threshold = 0.01,linear.output = TRUE, act.fct = RelU)\r\n",
    "    ```\r\n",
    "```r\r\n",
    "library(neuralnet)\r\n",
    "Var <- runif(50, 0, 10)\r\n",
    "trva <- data.frame(Var, Sin=sin(Var))\r\n",
    "tr <- trva[1:25,] # Training\r\n",
    "va <- trva[26:50,] # Validation\r\n",
    "form = as.formula(\"Sin~Var\")\r\n",
    "winit = runif(19, -1,1)\r\n",
    "nn.fit = neuralnet(form, data = tr, rep=10,hidden = c(2,2,2), threshold = 0.01,linear.output = TRUE,\r\n",
    "                   startweights = winit)\r\n",
    "plot(nn.fit, show.weights = TRUE)\r\n",
    "## Prediction\r\n",
    "yTestPred = compute(nn.fit, va)\r\n",
    "plot(prediction(x = nn.fit)$rep1)\r\n",
    "points(trva, col=2)\r\n",
    "## Test Results\r\n",
    "yTestPred$net.result\r\n",
    "plot(yTestPred$net.result)\r\n",
    "points(va$Sin, col=2)\r\n",
    "```\r\n",
    "<h3 id=\"2-em-b\">EM : Multivariate Bernoulli</h3>\r\n",
    "\r\n",
    "  - In case of Binary Data, use 'bernoulli' priors\r\n",
    "  - In case of Multiclass data, use 'dirichlet' priors!\r\n",
    "  - In contrast to the mixture of Gaussians, there are no singularities in which the likelihood function goes to infinity.This can be seen by noting that the\r\n",
    "likelihood function is bounded because $\\text{0 }\\le p(\\mathbf{x_n|\\mu_k}) \\le \\text{ 1}$\r\n",
    "  - $\\text{Log-Likelihood :} \\ln p(\\textbf{X}|\\mu, \\pi)  = \\displaystyle \\sum_{n=1}^{N} ln \\{\\sum_{k=1}^{K} \\pi_{k} p(\\mathbf{x}_n | \\mathbf{\\mu}_k)  \\}$ where $p(\\mathbf{x}_n | \\mathbf{\\mu}_k) = \\displaystyle \\prod_{i=1}^{D} \\mu_i^{x_i} \\left(1 - \\mu_i \\right)^{1 - x_i}$ i.e., $x_n$ datapoint with $D$ \"binary\" variables.\r\n",
    "  - Algo Steps:\r\n",
    "    1. Use $\\text{Eq. 9.54}$ to get the $\\textbf{Complete Data Log Likelihood}$\r\n",
    "    2. Use $\\text{Eq. 9.56}$ to get the responsibility($\\gamma(z_{nk})$) of component $k$ given datapoint $\\mathbf{x}_n$ for which we get a `resp` matrix of dim $[N,K]$\r\n",
    "    3. Now is M-Step:    \r\n",
    "      3.1. Calculate $N_k$ using $\\text{Eq. 9.57}$. : `N_k = colSums(resp)`    \r\n",
    "      3.2. Use $\\text{Eq. 9.58}$ to get $\\bar{x}$.: `x_k = t(resp)%*%x # same dimension as mu`    \r\n",
    "      3.3. Update $\\mathbf{\\mu}_k = \\bar{x}$: `mu[k,] = x_k[k,]/N_k[k] # Using eq. 9.58 and 9.59`    \r\n",
    "      3.3. Update $\\pi_k$ using __Eq 9.60__ : `pi = N_k/N # eq. 9.60` \r\n",
    "\r\n",
    "```r\r\n",
    "library(dplyr) ## For Piping\r\n",
    "set.seed(1234567890)\r\n",
    "max_it = 100\r\n",
    "min_change = 0.1\r\n",
    "N = 1000\r\n",
    "D= 10\r\n",
    "x = matrix(nrow = N,ncol = D)\r\n",
    "true_pi = vector(length = 3)\r\n",
    "true_mu = matrix(nrow = 3, ncol=D)\r\n",
    "true_pi = c(1/3,1/3,1/3)\r\n",
    "true_mu[1,] = c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)\r\n",
    "true_mu[2,] = c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)\r\n",
    "true_mu[3,] = c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)\r\n",
    "plot(true_mu[1,], type=\"o\", col=\"blue\", ylim=c(0,1))\r\n",
    "points(true_mu[2,], type=\"o\", col=\"red\")\r\n",
    "points(true_mu[3,], type=\"o\", col=\"green\")\r\n",
    "# Training Data\r\n",
    "for(n in 1:N){\r\n",
    "   k = sample(1:3, 1, prob = true_pi)\r\n",
    "   for(d in 1:D){\r\n",
    "     x[n,d] = rbinom(1,1,true_mu[k,d])\r\n",
    "   }\r\n",
    "}\r\n",
    "K = 3 # Guessed components\r\n",
    "pi = vector(length = K)\r\n",
    "mu = matrix(nrow=K, ncol=D)\r\n",
    "llik = vector(length = max_it)\r\n",
    "# Random Initialization of the Parameters\r\n",
    "pi = runif(K,0.49,0.51)\r\n",
    "pi = pi/sum(pi)  # Sum = 1 for 'Mixing Coefficients'\r\n",
    "for(k in 1:K){\r\n",
    "    mu[k,] = runif(D, 0.49,0.51)\r\n",
    "}\r\n",
    "for(it in 1:max_it){\r\n",
    "  plot(mu[1,], type=\"o\", col=\"blue\", ylim=c(0,1))\r\n",
    "  points(mu[2,], type=\"o\", col=\"red\")\r\n",
    "  points(mu[3,], type=\"o\", col=\"green\")\r\n",
    "  # points(mu[3,], type=\"o\", col=\"black\")\r\n",
    "  Sys.sleep(0.5)\r\n",
    "  # E-Step\r\n",
    "  # Measure Log-Likelihood\r\n",
    "  lik1 = function(n,K,pi,mu){\r\n",
    "    lik = NA\r\n",
    "    # Log-likelihood for 1 datapoint\r\n",
    "    for(k in 1:K){\r\n",
    "      t1 = log(pi[k]) # Log(pi_k)\r\n",
    "      t2 = x[n,]*log(mu[k,]) # x_{nd} * log(mu_{kd}) # for d columns\r\n",
    "      t3 = (1-x[n,])*log(1 - mu[k,]) # (1 - x_{nd}) * log(1 - mu_{kd}) # for d columns\r\n",
    "      lik[k] = t1 + sum(t2+t3)\r\n",
    "    }\r\n",
    "    return(sum(lik))\r\n",
    "  }\r\n",
    "  llik[it] = sapply(1:N, FUN = lik1, K=K,pi=pi,mu=mu) %>% sum ## Eq 9.54\r\n",
    "  cat(\"iteration: \", it, \" Lig-Likelihood: \", llik[it], \"\\n\")\r\n",
    "  # Calculating responsibility  resp = matrix(nrow = N, ncol = K) # holds eq. 9.56 variable i.e., responsibility\r\n",
    "  for(n in 1:N){\r\n",
    "    total_k = NA\r\n",
    "    for(k in 1:K){\r\n",
    "      t1 = log(pi[k])\r\n",
    "      t2 = (x[n,]*log(mu[k,]))\r\n",
    "      t3 = (1-x[n,])*log(1 - mu[k,])\r\n",
    "      total_k[k] = t1 + sum(t2+t3)\r\n",
    "    }\r\n",
    "      total_k = exp(total_k)\r\n",
    "      total_k = total_k/sum(total_k)\r\n",
    "      resp[n,] = total_k\r\n",
    "  }\r\n",
    "  # M- Step\r\n",
    "  N_k = colSums(resp) # 9.57\r\n",
    "  cat(\"N_k: \", N_k, \"\\n\")\r\n",
    "  x_k = t(resp)%*%x # same dimension as mu\r\n",
    "  # Update mu\r\n",
    "  for(k in 1:K){\r\n",
    "    mu[k,] = x_k[k,]/N_k[k] # Using eq. 9.58 and 9.59\r\n",
    "  }\r\n",
    "  # Update pi\r\n",
    "  pi = N_k/N # eq. 9.60\r\n",
    "  if(it>1){\r\n",
    "  if(abs(llik[it-1] - llik[it])<min_change)break\r\n",
    "  else{print(round(abs(llik[it-1] - llik[it]), digits = 3))}\r\n",
    "    }\r\n",
    "}\r\n",
    "``` \r\n",
    "\r\n",
    "<h3 id=\"2-em-g\">EM: Mixture of Gaussians</h3>\r\n",
    "\r\n",
    "  - Initial $\\mu$ and $\\Sigma$ can be obtained using `kmeans`\r\n",
    "  - $\\text{Log-Likelihood :} \\ln p(\\textbf{X}|\\mu, \\pi,\\Sigma)  = \\displaystyle \\sum_{n=1}^{N}  \\sum_{k=1}^{K} \\{ \\ln \\pi_{k} +\\ln \\mathcal{N}(\\mathbf{x}_n | \\mathbf{\\mu}_k, \\Sigma_k)  \\}$ \r\n",
    "  - **E-Step**:\r\n",
    "    - Evaluate the responsibility using **Eq. 9.23**. Implemented by the function `e_step`\r\n",
    "  - **M-Step**:\r\n",
    "    - First get $N_k$ for **Eq. 9.27** using `colSums(e_step(...))` saved in `N_k` variable\r\n",
    "    - Get Updates $\\mathbf{\\mu}_k^{new}$ using **Eq. 9.24** and function `update_mu`\r\n",
    "    - Get Updated $\\mathbf{\\pi}_k^{new}$ using **Eq. 9.26** and `update_pi` function.\r\n",
    "    - **CAREFUL!** For getting $\\Sigma_K^{new}$, we have two steps:\r\n",
    "      - Use function `cov_k` to get $\\gamma(\\textit{z}_{nk})(\\mathbf{x}_n - \\mathbf{\\mu}_k^{new})\\left(\\mathbf{x}_n - \\mathbf{\\mu}_k^{new} \\right)^{T}$ for only 1 datapoint.\r\n",
    "      - Now apply `cov_k` function on all data-points and sum them up: `sigmaK = sapply(1:N, FUN = cov_k, df = df, K=k,z = z) %>% rowSums`\r\n",
    "      - Re-arrange the co-variance matrix: `sigmaK = matrix(sigmaK, ncol=D, byrow = T)`\r\n",
    "      - In case We need it to be only-diagonal matrix: `sigmA[k,,] = diag(diag(sigmaK)/N_k[k]) ## Only Diagonals`\r\n",
    "      - If we dont want it to be non-diagonal: `sigmA[k,,] = sigmaK/N_k[k]) ## We need to divide by N_k[k]` \r\n",
    "```r\r\n",
    "library(dplyr) ## For Piping\r\n",
    "library(mvtnorm)\r\n",
    "dataEM = read.table(paste0(file_path, \"dataEM.txt\"))\r\n",
    "N = dim(dataEM)[1]\r\n",
    "D = dim(dataEM)[2] # Can be columns\r\n",
    "df  = as.data.frame(dataEM)\r\n",
    "\r\n",
    "# 'x_density' and 'CDLL' need to be outside EM function as they are helper\r\n",
    "# Observe Complete Data Log Likelihood\r\n",
    "x_density = function(n,df,K,pi,mu,sigmA){\r\n",
    "  vK = NA\r\n",
    "  for(k in 1:K){\r\n",
    "    \r\n",
    "    vK[k]= log(pi[k]) + dmvnorm(df[n,], mean = mu[k,], sigma = sigmA[k,,], log = TRUE)\r\n",
    "  }\r\n",
    "  return(sum(vK))\r\n",
    "}\r\n",
    "CDLL = function(df, K,pi,mu,sigmA){\r\n",
    "  loglik = sapply(1:N, FUN = x_density, df,K,pi,mu,sigmA) %>% sum\r\n",
    "  return(loglik)\r\n",
    "}\r\n",
    "# EM-Function\r\n",
    "EM = function(df,N,D,K,pi,mu,sigmA){\r\n",
    "  # E-Step\r\n",
    "  e_step = function(df,N,K, pi, mu, sigmA){\r\n",
    "    z = matrix(NA, nrow = N, ncol = K)\r\n",
    "    for(n in 1:N){\r\n",
    "      for(k in 1:K){\r\n",
    "        # z[n,k] is the numerator in eq. 9.23\r\n",
    "        z[n,k] =  pi[k]*dmvnorm(df[n,], mean = mu[k,],sigma = sigmA[k,,]) \r\n",
    "      }\r\n",
    "      \r\n",
    "      z[n,] = z[n,]/sum(z[n,]) # Eq 9.23\r\n",
    "    }\r\n",
    "    return(z)\r\n",
    "  }\r\n",
    "  z = e_step(df=df,N, K=K, pi=pi,mu = mu, sigmA = sigmA)\r\n",
    "  N_k = colSums(z,na.rm = TRUE)\r\n",
    "  ##M - step\r\n",
    "  # pi\r\n",
    "  update_pi = function(z, N){\r\n",
    "    pi_m = colSums(z)/N\r\n",
    "    pi_m = pi_m/sum(pi_m)\r\n",
    "    return(pi_m)\r\n",
    "  }\r\n",
    "  pi = update_pi(z,N)\r\n",
    "  # mu\r\n",
    "  update_mu = function(df, z, Nk){\r\n",
    "    mu_m = (t(df)%*%z)/Nk\r\n",
    "    return(as.matrix(t(mu_m)))\r\n",
    "  }\r\n",
    "  mu = update_mu(df, z, Nk = N_k)\r\n",
    "  # Update Sigma\r\n",
    "  cov_k = function(n,df, K, z){\r\n",
    "    df = as.matrix(df)\r\n",
    "    val = (df[n,]%*%t(df[n,]))*z[n,k]\r\n",
    "    return(as.matrix(val))\r\n",
    "  }\r\n",
    "  for(k in 1:K){\r\n",
    "    sigmaK = sapply(1:N, FUN = cov_k, df = df, K=k,z = z) %>% rowSums\r\n",
    "    sigmaK = matrix(sigmaK, ncol=D, byrow = T)\r\n",
    "    sigmA[k,,] = diag(diag(sigmaK)/N_k[k]) ## Only Diagonals\r\n",
    "  }\r\n",
    "  llik = CDLL(df,K,pi,mu,sigmA) ## CDLL is designed as a global function\r\n",
    "  return(list(pi = pi,mu = mu, sigma = sigmA, LLik = llik))\r\n",
    "}\r\n",
    "## Function to run EM Algorithm\r\n",
    "RUN_EM = function(df,N,D,K,pi,mu,sigmA,max_it,delta){\r\n",
    "  llik = vector(length = max_it)\r\n",
    "  for(it in 1:max_it){\r\n",
    "    if(it==1){\r\n",
    "      llik[it] = CDLL(df,K,pi,mu,sigmA)\r\n",
    "      result = EM(df,N,D,K,pi,mu,sigmA)\r\n",
    "      pi = result$pi\r\n",
    "      mu = result$mu\r\n",
    "      sigmA = result$sigma\r\n",
    "    }\r\n",
    "    else{\r\n",
    "      result = EM(df,N,D,K,pi,mu,sigmA)\r\n",
    "      pi = result$pi\r\n",
    "      mu = result$mu\r\n",
    "      sigmA = result$sigma \r\n",
    "      llik[it] = result$LLik\r\n",
    "      if( (abs(llik[it] - llik[it-1])<delta) )break\r\n",
    "    }\r\n",
    "  }\r\n",
    "  return(list(pi = pi,mu = mu, sigma = sigmA, LLik = llik, iterations = it))\r\n",
    "}\r\n",
    "## RUN EXPERIMENT\r\n",
    "K = 2\r\n",
    "D = 2 # Columns \r\n",
    "mu = matrix(nrow = K, ncol = D)\r\n",
    "sigmA = array(NA,dim=c(K,2,2)) # K X [D X D]\r\n",
    "for(k in 1:K){\r\n",
    "  mu[k,] = runif(D, 0,5)\r\n",
    "  sigmA[k,,] = diag(rep(1,D))\r\n",
    "}\r\n",
    "pi = runif(K,0.4,0.6)\r\n",
    "pi = pi/sum(pi)\r\n",
    "max_it = 100\r\n",
    "delta = 0.1\r\n",
    "result_2 = RUN_EM(df,N,D,K,pi,mu,sigmA,max_it,delta)\r\n",
    "# Plot\r\n",
    "par(mfrow=c(1,3))\r\n",
    "plot(result_2$LLik[2:result_2$iterations], col=2, type='l') # Plot only Non-Zero Log-Likelihoods\r\n",
    "plot(result_3$LLik[1:result_3$iterations], col=3, type='l')\r\n",
    "plot(result_4$LLik[2:result_4$iterations], col=4, type='l')\r\n",
    "# Calculate Bayesian Info Criteria\r\n",
    "BIC = function(LL,M,N){return(LL - (0.5*M/log(N)))}\r\n",
    "BIC2 = BIC(LL = result_2$LLik[result_2$iterations], M=2, N=600)\r\n",
    "```\r\n",
    "\r\n",
    "\r\n",
    "<h3 id=\"2-gam\">GAM(Generalized Additive Models)</h3>\r\n",
    "\r\n",
    "- Notes for Library `mgcv`\r\n",
    "  - Non-Linear Effects can be entered via `s` or `te`: \r\n",
    "  - Use `method = \"REML\"`\r\n",
    "  - always look at `gam.check()`\r\n",
    "  - Parameters: \r\n",
    "    - `s` in `gam(y~x0+s(x1)+s(x2,x3))` is the smoothing function:\r\n",
    "      - `k` specifies dimensions to represent smoothing term: ALWAYS equal to the length: `Mortality~Year+s(Week, k = length(unique(data1$Week)))`\r\n",
    "      - `bs` specifies the penalized smoothing basis: `Mortality~s(Week, k = length(unique(data1$Week)),bs='cc')`\r\n",
    "        - `bs=\"tp\"` :  for **Thin Plate Splines**\r\n",
    "        - `bs=\"cr\"` : for **Cubic Splines**\r\n",
    "        - `bs=\"cc\"` : for **cyclic cubuc splines**. Useful ifa cov shows cyclical trend like 'Year','Month'\r\n",
    "        - **_Generalized Cross Validation Score_** is given within `summary(model)`'s GCV value. Use `method=\"GCV.Cp\"` while fitting. \r\n",
    "\r\n",
    "```r\r\n",
    "library(mgcv)\r\n",
    "form.1 = as.formula(\"Mortality~Year+s(Week, k = length(unique(data1$Week)))\")\r\n",
    "fit1 = gam(form.1, family = gaussian(), data=data1, method = 'REML')\r\n",
    "for.2 = as.formula(\"Mortality~Year+s(Week, k = length(unique(data1$Week)))+\r\n",
    "                  s(Year, Influenza)\")\r\n",
    "fit2 = gam(for.2, family = gaussian(), data=data1, method = 'REML')\r\n",
    "# Gives siginificance of smooth terms and variance of the model\r\n",
    "summary(fit1) \r\n",
    "# Plotting the Residuals\r\n",
    "par(mfrow = c(2,2))\r\n",
    "gam.check(fit1)\r\n",
    "# Plot the Spline Component\r\n",
    "plot(fit1)  \r\n",
    "plot(fit1, residuals=T) # With Residuals\r\n",
    "# Compare Models\r\n",
    "anova(fit1,fit2, fit3, test = 'Chisq')\r\n",
    "AIC(fit1,fit2, fit3) # Choose with lowest AIC\r\n",
    "```\r\n",
    "<h3 id=\"2-kde\">Kernel Classification(using Kernel Density Estimation)</h3>    \r\n",
    "\r\n",
    "```r\r\n",
    "# January-2021 exam\r\n",
    "# data generation\r\n",
    "N_class1 <- 1500\r\n",
    "N_class2 <- 1000\r\n",
    "data_class1 <- NULL\r\n",
    "for(i in 1:N_class1){\r\n",
    "  a <- rbinom(n = 1, size = 1, prob = 0.3)\r\n",
    "  b <- rnorm(n = 1, mean = 15, sd = 3) * a + (1-a)*rnorm(n = 1, mean = 4, sd = 2)\r\n",
    "  data_class1 <- c(data_class1,b)\r\n",
    "}\r\n",
    "data_class2 <- NULL\r\n",
    "for(i in 1:N_class2){\r\n",
    "  a <- rbinom(n = 1, size = 1, prob = 0.4)\r\n",
    "  b <- rnorm(n = 1, mean = 10, sd = 5) * a + (1-a) * rnorm(n = 1, mean = 15, sd = 2)\r\n",
    "  data_class2 <- c(data_class2,b)\r\n",
    "}\r\n",
    "# y=1,0\r\n",
    "df1 = data.frame(cbind(data_class1, 1))\r\n",
    "df2 = data.frame(cbind(data_class2,0))\r\n",
    "# Prior Probabilities\r\n",
    "p_1 = 1500/2500\r\n",
    "P_2 = 1 - p_1\r\n",
    "# Define the Kernel\r\n",
    "kernel =  function(x,h){return(exp(- (x/h)^2))}\r\n",
    "xs = seq(-5,25,0.1) # Data to be tested on\r\n",
    "kern.prob = matrix(NA, nrow = length(xs), ncol = 2) # For Storing Posterior Class probability\r\n",
    "for(i in 1: length(xs)){\r\n",
    "  x = xs[i]\r\n",
    "  diff_1 = x - df1[,1] # For class-1 only use class-1 data\r\n",
    "  diff_2 = x- df2[,1] # For class-2 only use class-2 data\r\n",
    "  p1 = mean(sapply(diff_1, FUN = kernel, h = 5)) # P(X=x|Y = 'class 1')\r\n",
    "  p2 = mean(sapply(diff_2, FUN = kernel, h = 5)) # P(X=x|Y = 'class 2')\r\n",
    "  kern.prob[i,1] = (p1*p_1)/((p1*p_1) + (p2* P_2) ) # Applying Bayes Theorem\r\n",
    "  kern.prob[i,2] = (p2*P_2)/((p1*p_1) + (p2* P_2) ) # this is also equal to 1 - kern.prob[i,1]\r\n",
    "}\r\n",
    "# Check the Plots\r\n",
    "plot(x=xs, y = kern.prob[,1], type='l', ylim=c(0,1))\r\n",
    "lines(x=xs, y=kern.prob[,2],type='l', col=3)\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-lda\">LDA(Manual implementation can be extended to QDA and Naive Bayes)</h3>  \r\n",
    "\r\n",
    "  - Inputs are assumed to be **CONTINUOUS**\r\n",
    "  - All Classes share the same covariance matrix(In QDA, they don't)\r\n",
    "  - Because of 'common covariance matrix' assumption, the decision boundary corresponding to Posterior Probability $p(C_k|\\textbf{x})$ are constant and can be respresented by  a linear function of $\\textbf{x}$ i.e., \"decision boundaries are linear in input space\".\r\n",
    "  - Since, we estimate the parameters using MLE, the approach is susceptible to 'outliers'.\r\n",
    "  - While plotting, make the `xlim` and `ylim` of `generated data` same as Old Data.\r\n",
    "  - Use only diagonal elements of pooled-covariance matrix to perform Naive Bayes.\r\n",
    "  - Use different covariance matrix for each class to perform QDA \r\n",
    "\r\n",
    "```r\r\n",
    "library(MASS) # Step-1: Library\r\n",
    "crabs = as.data.frame(read.csv(paste0(file_path,'australian-crabs.csv')))\r\n",
    "crabs$sex = as.factor(crabs$sex) # Step-0:Convert the target to factor\r\n",
    "# Plot for Classes\r\n",
    "with(crabs, plot(CL, RW, main = \"Main\", type = \"n\"))\r\n",
    "with(subset(crabs, sex='Male'), points(CL, RW, col='blue'))\r\n",
    "with(subset(crabs, sex!='Male'), points(CL, RW, col='red'))\r\n",
    "legend(\"topleft\", pch = 1, col = c(\"blue\", \"red\"), legend = c(\"Male\", \"Female\"))\r\n",
    "form = as.formula('sex~CL+RW') # Step-2 formulation\r\n",
    "lda.model = lda(form, crabs, prior = c(0.1,0.9)) # Step-3, prior order as 'factor' order\r\n",
    "crabs[,'predictions'] = predict(lda.model,crabs)$class # Step-4\r\n",
    "# Plot- Multiple Predictions, Multiple CLasses\r\n",
    "par(mfrow = c(1, 2), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))\r\n",
    "with(crabs, {\r\n",
    "  plot(CL, RW, main = \"Prior\", type = \"n\")\r\n",
    "  with(subset(crabs, sex='Male'), points(CL, RW, col='blue'))\r\n",
    "  with(subset(crabs, sex!='Male'), points(CL, RW, col='red'))\r\n",
    "  legend(\"topleft\", pch = 1, col = c(\"blue\", \"red\"), legend = c(\"Male\", \"Female\"))\r\n",
    "  plot(CL, RW, main = \"Posterior\", type = \"n\")\r\n",
    "  with(subset(crabs, predictions='Male'), points(CL, RW, col='blue'))\r\n",
    "  with(subset(crabs, predictions!='Male'), points(CL, RW, col='red'))\r\n",
    "  legend(\"topleft\", pch = 1, col = c(\"blue\", \"red\"), legend = c(\"Male\", \"Female\"))\r\n",
    "})\r\n",
    "# Draw discriminant Line\r\n",
    "np = lda.model$N\r\n",
    "nd.x <- seq(from = min(crabs$RW), to = max(crabs$RW), length.out = np)\r\n",
    "nd.y <- seq(from = min(crabs$CL), to = max(crabs$CL), length.out = np)\r\n",
    "nd <- expand.grid(RW = nd.x, CL = nd.y)\r\n",
    "prd <- as.numeric(predict(lda.model, newdata = nd)$class)\r\n",
    "plot(crabs[, c('RW','CL')], col = crabs$sex) # Make Sure X and Y are ordered correctly\r\n",
    "contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), \r\n",
    "        levels = c(1,2), add = TRUE, drawlabels = FALSE)\r\n",
    "\r\n",
    "# Manual LDA\r\n",
    "# Compute Mean, covMat, priorProb\r\n",
    "setosa = as.data.frame(iris2[iris2$Species == 'setosa',])\r\n",
    "versicolor = as.data.frame(iris2[iris2$Species == 'versicolor',])\r\n",
    "virginica = as.data.frame(iris2[iris2$Species == 'virginica',])\r\n",
    "## Means, COvs and Prior Probs\r\n",
    "cov_names = c('Sepal.Width', 'Sepal.Length')\r\n",
    "mu_setosa = as.matrix(colMeans(setosa[,cov_names]))\r\n",
    "mu_ver = as.matrix(colMeans(versicolor[,cov_names]))\r\n",
    "mu_vir = as.matrix(colMeans(virginica[,cov_names]))\r\n",
    "cov_setosa = cov(setosa[,cov_names])\r\n",
    "cov_versi = cov(versicolor[,cov_names])\r\n",
    "cov_virgi = cov(virginica[,cov_names])\r\n",
    "prop_setosa = 50/150\r\n",
    "prob_ver = 50/150\r\n",
    "prop_virg = 1-(prop_setosa+prob_ver)\r\n",
    "## Pooled Covariance: Bishop eq 4.78(In case of QDA dont use pooled)\r\n",
    "S = as.matrix((prop_setosa*cov_setosa)+(prop_virg*cov_virgi)+(prob_ver*cov_versi))\r\n",
    "s_inv = as.matrix(solve(S))\r\n",
    "# Calculate Discriminant FUnction\r\n",
    "pred = matrix(NA, nrow = 150, ncol = 3)\r\n",
    "p_j = c(prop_setosa,prob_ver,prop_virg)\r\n",
    "mus = cbind(mu_setosa,mu_ver, mu_vir)\r\n",
    "for(i in 1:150){\r\n",
    "  for(j in 1:3){\r\n",
    "    pred[i,j] =as.matrix(iris2[i,cov_names])%*%s_inv%*%mus[,j] - 0.5*t(mus[,j])%*%s_inv%*%mus[,j] + log(p_j[j])\r\n",
    "  }\r\n",
    "}\r\n",
    "classes = c('setosa','versicolor','virginica')\r\n",
    "actual_predictions = matrix(NA,nrow=150,ncol=1)\r\n",
    "for(i in 1:150){\r\n",
    "  actual_predictions[i] =classes[which.max(pred[i,])]\r\n",
    "}\r\n",
    "# Sampling From New Sample\r\n",
    "library(mvtnorm)\r\n",
    "## Generate synthetic data for each class i.e., x ~ N(mu_k,Sigma)\r\n",
    "# Ex for setosa: rmvnorm(50, mean = mu_setosa, sigma = S)\r\n",
    "syn_data = rbind(cbind(rmvnorm(50, mean = mu_setosa, sigma = S), 'setosa'),\r\n",
    "cbind(rmvnorm(50, mean = mu_ver, sigma = S), 'versicolor'),\r\n",
    "cbind(rmvnorm(50, mean = mu_vir, sigma = S), 'virginica'))\r\n",
    "colnames(syn_data) = c('Sepal.Width','Sepal.Length','Species')\r\n",
    "syn_data = as.data.frame(syn_data)\r\n",
    "## Plot and comparing Synthetic Data with actual data\r\n",
    "par(mfrow = c(1, 2), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))\r\n",
    "with(iris, plot(Sepal.Width, Sepal.Length, main = \"Main\", type = \"n\"))\r\n",
    "with(subset(iris, Species='setosa'), points(Sepal.Width, Sepal.Length, col='blue'))\r\n",
    "with(subset(iris, (Species!='setosa' & Species !='versicolor')), \r\n",
    "     points(Sepal.Width, Sepal.Length, col='red'))\r\n",
    "with(subset(iris, (Species!='setosa' & Species !='virginica')), \r\n",
    "     points(Sepal.Width, Sepal.Length, col='green'))\r\n",
    "legend(\"topleft\", pch = 1, col = c(\"blue\", \"red\", 'green'), legend = c(\"setosa\", \"virginica\",'versicolor'))\r\n",
    "with(syn_data, plot(Sepal.Width, Sepal.Length, main = \"Synthetic Data\", type = \"n\"))\r\n",
    "with(subset(syn_data, Species='setosa'), points(Sepal.Width, Sepal.Length, col='blue'))\r\n",
    "with(subset(syn_data, (Species!='setosa' & Species !='versicolor')), \r\n",
    "     points(Sepal.Width, Sepal.Length, col='red'))\r\n",
    "with(subset(syn_data, (Species!='setosa' & Species !='virginica')), \r\n",
    "     points(Sepal.Width, Sepal.Length, col='green'))\r\n",
    "legend(\"topleft\", pch = 1, col = c(\"blue\", \"red\", 'green'), legend = c(\"setosa\", \"virginica\",'versicolor'))\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-qda\">QDA</h3>   \r\n",
    "\r\n",
    "  - Refer LDA codes for Templates. For Manual application use LDA template and modify individual sigma, refer ISLR eq. 4.19\r\n",
    "\r\n",
    "```r\r\n",
    "### QDA\r\n",
    "form = as.formula('sex~CL+RW') # Step-2 formulation\r\n",
    "qda.model = qda(form, crabs, prior= c(0.5,0.5)) # Step-3\r\n",
    "crabs[,'qdapredictions'] = predict(qda.model,crabs)$class\r\n",
    "# Misclassification Error\r\n",
    "1 - mean(crabs$sex == crabs$qdapredictions)\r\n",
    "# Draw discriminant Line\r\n",
    "np = qda.model$N\r\n",
    "nd.x <- seq(from = min(crabs$RW), to = max(crabs$RW), length.out = np)\r\n",
    "nd.y <- seq(from = min(crabs$CL), to = max(crabs$CL), length.out = np)\r\n",
    "nd <- expand.grid(RW = nd.x, CL = nd.y)\r\n",
    "prd <- as.numeric(predict(qda.model, newdata = nd)$class)\r\n",
    "plot(crabs[, c('RW','CL')], col = crabs$qdapredictions) # Make Sure X and Y are ordered correctly\r\n",
    "contour(x = nd.x, y = nd.y, z = matrix(prd, nrow = np, ncol = np), \r\n",
    "        levels = c(1,2), add = TRUE, drawlabels = FALSE)\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-dt\">Decision Trees: Classification/Regression</h3>\r\n",
    "\r\n",
    "  - Lesser the _residual Mean deviance_.better the model. _Residual Mean Deviance_ = $\\frac{\\text{deviance}}{n- |T_o|}$ where $n= \\text{no. of data points}$ and $T_o=\\text{No. of Terminal Nodes}$\r\n",
    "  - _Deviance_ = $-2 \\sum_m \\sum_k n_{mk} \\cdot log \\hat{p}_{mk}$ where $n_{mk}$ is the no. of obs in the *m*th terminal node belonging to the *k*th class\r\n",
    "  - Optimal Tree can be selected using:    \r\n",
    "    1. `cv.tree()`\r\n",
    "    2. Trai/Test Validation\r\n",
    "  - In *Regression* Trees, the deviance is 'Sum of Square Errors' for the Tree!.  \r\n",
    "\r\n",
    "```r\r\n",
    "# Classification\r\n",
    "credit = read_excel(paste0(file_path,'creditscoring.xls'))\r\n",
    "credit$good_bad = as.factor(credit$good_bad) # Labels need to be factored!\r\n",
    "library(tree) # Call the Library \r\n",
    "form = as.formula(\"good_bad~.\") # Create the formula\r\n",
    "# NOTE: split = 'gini' for gini-index\r\n",
    "train.tree.dev = tree(form, data = train, split = 'deviance') # Fit the model on 'Train'\r\n",
    "train.tree.gini = tree(form, data=train, split='gini')\r\n",
    "summary(train.tree.dev);summary(train.tree.gini)\r\n",
    "## Choosing Optimal Tree\r\n",
    "# 1. By Penalizing\r\n",
    "# ------------------\r\n",
    "fit = tree(form, data = train)\r\n",
    "cv.res = cv.tree(fit, FUN = prune.misclass) # By default FUN = 'deviance'\r\n",
    "par(mfrow=c(1,2))\r\n",
    "plot(cv.res$size, cv.res$dev, type=\"b\",\r\n",
    "     col=\"red\", main = 'CV-Error: Misclassification', xlab='Terminal Nodes')\r\n",
    "plot(log(cv.res$k), cv.res$dev,\r\n",
    "     type=\"b\", col=\"red\", main = 'CV-Error: Misclassification', xlab='Cost-Complexity Parameter')\r\n",
    "# 5 Terminal Node looks to provide the lowest Misclassification rate.\r\n",
    "# 'cv.res$size' returns the no of terminal nodes.\r\n",
    "# NOTE: 'cv.res$dev' will return the cross-validation error and not 'deviance' which in this case \r\n",
    "#is misclassifcation error\r\n",
    "# 2. Train/Val Method\r\n",
    "# ------------------\r\n",
    "trainScore = testScore = rep(0,9)\r\n",
    "for(i in 2:9){\r\n",
    "  prunedTree = prune.tree(fit, best = i)\r\n",
    "  # Put 'type' = 'class' in case of predictions\r\n",
    "  pred = predict(prunedTree, newdata=val, type='tree')# Returns Tree\r\n",
    "  trainScore[i] = deviance(prunedTree)\r\n",
    "  testScore[i] = deviance(pred)\r\n",
    "}\r\n",
    "plot(2:9, trainScore[2:9], type=\"b\", col=\"red\", ylim = c(min(trainScore,testScore), \r\n",
    "                                                         max(trainScore,testScore)),main='Train Score', ylab='Deviance', xlab='No. of Nodes')\r\n",
    "points(2:9, testScore[2:9], type=\"b\", col=\"blue\")\r\n",
    "opTimalTree = prune.tree(fit, best=5)\r\n",
    "summary(opTimalTree)\r\n",
    "# State the following in the report:Variables selected, # Terminal Nodes, Res. mean deviance, Misclassification error\r\n",
    "# Estimate Misclassification Rate for Test data. NOTE: type='class'\r\n",
    "testPred = predict(opTimalTree, newdata = test, type='class')\r\n",
    "1 - mean(test$good_bad == testPred)\r\n",
    "\r\n",
    "##### Regression Trees\r\n",
    "formreg = as.formula(\"amount~.\")\r\n",
    "fitReg = tree(formreg, data = train)\r\n",
    "trY = predict(fitReg, newdata = val)\r\n",
    "sqrt(mean((val$amount - trY)^2)) ## RMSE\r\n",
    "```\r\n",
    " \r\n",
    "\r\n",
    "<h3 id=\"2-splines\">Splines: Using base R Functions only</h3>\r\n",
    "\r\n",
    "  - Example below uses only basic r functions with `lm`\r\n",
    "  - Step 1: For order-M Spline We create upto M-1 `degree of freedom` variables\r\n",
    "  ```r\r\n",
    "  M=5\r\n",
    "  X = matrix(data$Day %>% poly(degree = M-1, raw = TRUE), ncol =M-1) ## KEEP `raw=TRUE` when using poly\r\n",
    "  ```\r\n",
    "  - Step-2: Create Truncated Power Basis functions $h(x,\\xi)$ for each knot!\r\n",
    "  ```r\r\n",
    "  # knot 洧래=75\r\n",
    "  h.75= ifelse(X[,1]>75, X[,4],0)\r\n",
    "  # If 洧래=80\r\n",
    "  h.80= ifelse(X[,1]>80, X[,4],0)\r\n",
    "  ```\r\n",
    "  - Step-3: Combine Knots and Splines\r\n",
    "  ```r\r\n",
    "  # knot 洧래=75\r\n",
    "  X = cbind(X,h.75)\r\n",
    "  # If knots: 洧래=75,洧래=80\r\n",
    "  X = cbind(X, h.75,h.80)\r\n",
    "  ```\r\n",
    "\r\n",
    "```r\r\n",
    "# Use basis function approach to fit an order-5 spline with a single knot 洧래=75 such that Day is the feature and Rate is the target variable.\r\n",
    "data = read.csv2(paste0(file_path,\"mortality_rate.csv\"))\r\n",
    "M=5\r\n",
    "X = matrix(data$Day %>% poly(degree = M-1, raw = TRUE), ncol =M-1) ## KEEP `raw=TRUE` when using poly\r\n",
    "h.75= ifelse(X[,1]>75, X[,4],0)\r\n",
    "X = cbind(X,h.75)\r\n",
    "colnames(X) = as.character(seq(1:5))\r\n",
    "df = data.frame(cbind(X,data$Rate))\r\n",
    "colnames(df)\r\n",
    "spline.fit = lm(\"V6~.\", data = df)\r\n",
    "yPred = predict(spline.fit, data = df)\r\n",
    "plot(x = data$Day, y = data$Rate)\r\n",
    "points(x = data$Day,y = yPred, col=2)\r\n",
    "```\r\n",
    "<h3 id=\"2-splines2\">Splines: Using splines library</h3> \r\n",
    "\r\n",
    "```r\r\n",
    "# Using Library Splines\r\n",
    "library(splines)\r\n",
    "data = read.csv2(paste0(file_path,\"mortality_rate.csv\"))\r\n",
    "f = as.formula(\"Rate~bs(Day, df=4, knots = 75)\") ## Because order is 5!\r\n",
    "spline.fit = lm(f, data = data)\r\n",
    "yPred = predict(spline.fit, newdata = data)\r\n",
    "plot(x = data$Day, y = data$Rate)\r\n",
    "points(x = data$Day,y = yPred, col=2)\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-logistic\">Logistic Regression</h3> \r\n",
    " \r\n",
    "  - for prediction use `predict(glm.fits, type = 'response')` script. This generates probability for $\\textbf{p(Y=1|X)}$  \r\n",
    "  - $p(Y=1|\\mathbf{w,X}) = \\sigma{\\left(\\mathbf{w^T \\cdot X} \\right)}$ where $\\sigma(a) = \\frac{1}{1 + e^{-a}}$ with a linear decision boundary  at $\\mathbf{w^T\\cdot X}= 0$\r\n",
    "  - Also $Y \\sim Bern(\\sigma(\\mathbf{w^T \\cdot X}))$\r\n",
    "\r\n",
    "```r\r\n",
    "spam$target = ifelse(spam$Spam==1, 'S','H') # Create a binary variable\r\n",
    "spam$target = as.factor(spam$target) # Convert to Factor\r\n",
    "# Check the factor\r\n",
    "contrasts(spam$target)\r\n",
    "cov=paste(covariates, collapse='+')\r\n",
    "form = as.formula(paste0('Spam~',cov))\r\n",
    "glm.fits = glm(form, data=train,family=\"binomial\") # Family should be 'binomial'\r\n",
    "test_pred = predict(glm.fits, type = 'response', newdata = test)\r\n",
    "test_pred = ifelse(test_pred>0.5,'Spam','Ham') # Y=1 for 'Spam'\r\n",
    "# Accuracy\r\n",
    "table(train_pred, train$target)\r\n",
    "# Misclassification\r\n",
    "1 - mean(train_pred == train$target)\r\n",
    "1 - mean(test_pred==test$target)\r\n",
    "# TPR,FPR,ROC\r\n",
    "pred_tree.prob = predict(opTimalTree, test, type='vector')[,'good'] # Get Class Probability\r\n",
    "# Calculate TPR, FPR\r\n",
    "get.rate = function(predicted.prob, actual, true_label, false_label, threshold){\r\n",
    "  prediction = ifelse(predicted.prob>threshold, true_label,false_label)\r\n",
    "  TPR = sum(prediction==true_label & actual == true_label)/sum(actual == true_label)\r\n",
    "  FPR = sum(prediction==true_label & actual == false_label)/ sum(actual == false_label)\r\n",
    "  return(c(TPR, FPR))\r\n",
    "}\r\n",
    "thresholds = seq(0.05,0.95,0.05)\r\n",
    "cols = c('threshold','TPR', 'FPR')\r\n",
    "tree.roc = matrix(data = NA, nrow = length(thresholds), ncol = 3)\r\n",
    "colnames(tree.roc) =  cols\r\n",
    "for(i in 1:length(thresholds)){\r\n",
    "  threshold = thresholds[i]\r\n",
    "  # Tree\r\n",
    "  res.tree = get.rate(pred_tree.prob, test$good_bad, 'good', 'bad', threshold)\r\n",
    "  tree.roc[i, cols] = c(threshold, res.tree)\r\n",
    "}\r\n",
    "# PLOT ROC\r\n",
    "plot(x = tree.roc[,'FPR'], y = tree.roc[,'TPR'], type='l',\r\n",
    "     xlab='FPR', ylab='TPR', col='red')\r\n",
    "```\r\n",
    "<h3 id=\"2-knn\">K Nearest Neighbors</h3> \r\n",
    "\r\n",
    "    - NOTE: How 'train' and 'validation' are evaluated. Param `k` decides on no. of neighbors. `d` decides Minkowski distance. `d=2` for `euclidean`\r\n",
    "    - theres no `predict`, use the data to be predicted in `test`   \r\n",
    "\r\n",
    "```r\r\n",
    "library(\"kknn\")\r\n",
    "# k=30\r\n",
    "# Target has value 0 or 1\r\n",
    "k.30.train = kknn(form, train=train,test=train,k=30,distance=2, kernel='rectangular') # Creating Model based on Training data as Validation\r\n",
    "k.30.test = kknn(form, train=train,test = test,k=30,distance=2, kernel='rectangular')\r\n",
    "train.pred = ifelse(k.30.train$fit>threshold,'S','H' ) # Script to modify the prediction\r\n",
    "print(table(Predicted=train.pred, Actual = train$target)) # Training Confusion\r\n",
    "print(table(Predicted=test.pred, Actual = test$target)) # Test Confusion\r\n",
    "1 - mean(train.pred==train$target) # Accuracy\r\n",
    "```\r\n",
    "<h3 id=\"2-ridge\">Ridge Regression</h3>\r\n",
    "\r\n",
    "  - In 'glmnet' 'alpha=0' is for ridge. \r\n",
    "  - Make sure that input data is in `matrix` format i.e., `glmnet(x = as.matrix(scaled_data[,coefs[2:64]]), y = as.matrix(scaled_data[,'Fat']),\r\n",
    "                   alpha=0, lambda = grid)`\r\n",
    "  - choosing good grid values: `grid <- 10 ^ seq (10, -2, length = 100)`\r\n",
    "  - use `type.measure=\"class\"` for getting misclassification error in case of classification task.\r\n",
    "  - We use 'MSE' because :  \r\n",
    "    1. It is the MLE of a normal distribution and we assume error to be normally distributed.   \r\n",
    "    2. MSE is a quadratic function i.e., it is easily differentiable and will always have a minima. \r\n",
    "  - Effective Degrees of freedom: $df(\\lambda) = tr(X(X'X + \\lambda I_p)^{-1} X')$  \r\n",
    "\r\n",
    "```r\r\n",
    "# Ridge Regression\r\n",
    "grid <- 10 ^ seq (10, -2, length = 100)\r\n",
    "ridge.fit = glmnet(x = as.matrix(scaled_data[,coefs[2:64]]), y = scaled_data[,'Fat'],alpha=0, lambda = grid)\r\n",
    "# 5 fold cv\r\n",
    "cv.5 = cv.glmnet(x = as.matrix(train[,coefs]), y = as.matrix(train[,'Fat']),alpha=0,lambda = grid,nfolds=10,type.measure = \"mse\") \r\n",
    "# Plot CV Results\r\n",
    "plot(log(cv.5$lambda), cv.5$cvm,pch=19,col='red',xlab = 'log(lambda)',ylab = cv.5$name)\r\n",
    "# Optimal lambda\r\n",
    "print(cv.5$lambda.min)\r\n",
    "```\r\n",
    "<h3 id=\"2-poisson\">Poisson Regression</h3>\r\n",
    "\r\n",
    "  - $p(y=Y|\\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y}}{Y!}$\r\n",
    "  - For Prediction of $\\hat{Y}$, we use $\\hat{Y} =  e^{\\mathbf{W^TX}}$ whete $\\lambda = \\mathbf{W^TX}$\r\n",
    "    ```r\r\n",
    "    yTestPred = predict(lasso.fit, newx = as.matrix(test[,c(1:8)]), type = \"response\")\r\n",
    "    yTestPred[1]\r\n",
    "    yHat = as.matrix(cbind(1,test[1,c(1:8)]))%*% matrix(coef(lasso.fit), ncol=1)\r\n",
    "    prediction = exp(yay)\r\n",
    "    ```\r\n",
    "\r\n",
    "<h3 id=\"2-ridge-optim\">Ridge Regression using Optim</h3>\r\n",
    "\r\n",
    "  - Use Block-1, Lab-1 Q2 as premise. Make sure to SCALE the data!!!\r\n",
    "  - Make sure that the `X`, `Y` and `Weights` are in `as.matrix` format.\r\n",
    "  - For Log-Likelihood, used Bishop eq.(3.11) and for applying ridge penalty used equation from pg.17 of `Lecture 1D,Block1`\r\n",
    "\r\n",
    "  ```r\r\n",
    "  # LogLikelihood = log P(Y|X,w,sigma)\r\n",
    "  Loglikelihood = function(x,y,w,sigma){\r\n",
    "  wTX = as.matrix(x)%*%w\r\n",
    "  loss = sum((wTX - as.matrix(y))^2)\r\n",
    "  N = dim(x)[1]\r\n",
    "  t1 = (N*0.5*(log(sigma)))\r\n",
    "  t2 = (N*0.5*log(2*pi))\r\n",
    "  t3 = (sigma*0.5*loss)\r\n",
    "  logLik = t1-t2 - t3\r\n",
    "  return(logLik)\r\n",
    "  }\r\n",
    "  ## Ridge Penalty\r\n",
    "  ridge = function(w,sigma,lambda, x,y){\r\n",
    "    log_lik = Loglikelihood(x,y,w,sigma)\r\n",
    "    penalty = lambda*(sum(w^2)) - log_lik\r\n",
    "    return(penalty)\r\n",
    "  }\r\n",
    "  ## Ridge Degree of Freedom\r\n",
    "  ridge.df = function(x,lambda){\r\n",
    "    x = as.matrix(x)\r\n",
    "    N = dim(x)[2] # Covariate count\r\n",
    "    Ip = diag(N)\r\n",
    "    inv_data = solve(t(x)%*%x + (lambda*Ip))\r\n",
    "    df = x%*%inv_data%*%t(x)\r\n",
    "    trace = sum(diag(df))\r\n",
    "    return(trace)\r\n",
    "  }\r\n",
    "  ### AIC criteria\r\n",
    "  calculate_AIC = function(x,y,w,sigma, lambda){\r\n",
    "    log_lik = Loglikelihood(x,y,w,sigma)\r\n",
    "    N = dim(x)[1] # No of data points\r\n",
    "    df = ridge.df(x,lambda)\r\n",
    "    aic = (-2*log_lik/N) + (2*df/N) #(-2*Log-likelihood/N) + 2*(df/N)\r\n",
    "    return(aic)\r\n",
    "  }\r\n",
    "  # Calculate Train,Test Loss\r\n",
    "  for(lambda in c(1,100,1000)){\r\n",
    "    N = dim(train[,covs])[2]\r\n",
    "    initW = as.matrix(rnorm(N))\r\n",
    "    sigma=runif(1)\r\n",
    "    lambda=lambda\r\n",
    "    x = as.matrix(train[,covs])\r\n",
    "    y = as.matrix(train[,'motor_UPDRS'])\r\n",
    "    xTest = as.matrix(test[,covs])\r\n",
    "    yTest = as.matrix(test[,'motor_UPDRS'])\r\n",
    "    ridgeOpt = optim(initW, ridge,sigma=sigma,lambda=lambda,x=x,y=y,method=c(\"BFGS\"))\r\n",
    "    weights = as.matrix(ridgeOpt$par)\r\n",
    "    train_mse = mean((x%*%weights - y)^2);test_mse = mean((xTest%*%weights - yTest)^2)\r\n",
    "    print(paste0('Lambda: ', lambda));print(paste0('Train Error: ', train_mse))\r\n",
    "    print(calculate_AIC(x,y,w=weights,sigma, lambda)) # For AIC Calculation\r\n",
    "  }\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-lasso\">LASSO</h3>\r\n",
    "\r\n",
    "  - **Standardize** the data!   \r\n",
    "  - Almost same as Ridge. But, `df`(degree of freedom) is also covered.   \r\n",
    "  - Example of how to generate synthetic data is below.   \r\n",
    "\r\n",
    "```r\r\n",
    "  # Lasso Regression\r\n",
    "  lasso.fit = glmnet(x = as.matrix(train[,coefs]), y = as.matrix(train[,'Fat']),alpha=1,family=\"gaussian\")\r\n",
    "  # Plotting Lambda V/S Coefficients\r\n",
    "  plot(lasso.fit, xvar = \"lambda\", label = TRUE)\r\n",
    "  # Plot LASSO V/S df\r\n",
    "  plot(x = log(lasso.fit$lambda), y = lasso.fit$df)\r\n",
    "  ## Cross Validation\r\n",
    "  lasso.grid <- 10 ^ seq (10, -2, length = 100)\r\n",
    "  # 5 fold cv\r\n",
    "  cv.5 = cv.glmnet(x = as.matrix(train[,coefs]), y = as.matrix(train[,'Fat']),alpha=1,family=\"gaussian\",\r\n",
    "                    lambda = lasso.grid,nfolds=10,type.measure = \"mse\")\r\n",
    "  # Plot CV Results: MSE V/S Log(Lambda)\r\n",
    "  plot(log(cv.5$lambda), cv.5$cvm,pch=19,col='red',xlab = 'log(lambda)',ylab = cv.5$name)\r\n",
    "  # Optimal lambda\r\n",
    "  print(cv.5$lambda.min)\r\n",
    "  ### Generating synthetic test data\r\n",
    "  X = as.matrix(test[,coefs]);Y = as.matrix(test[,'Fat']) # Actual Test Data\r\n",
    "  lasso.fit = glmnet(x = as.matrix(train[,coefs]), y = as.matrix(train[,'Fat']),alpha=1, lambda = cv.5$lambda.min) # LASSO with best fit\r\n",
    "  betas = as.vector(coef(lasso.fit)[-1,]) # Remove intercept and fetch beta coefficients\r\n",
    "  residual = train$Fat - as.matrix(train[,coefs])%*% betas;stdev = sd(residual) # Residual and st dev\r\n",
    "  yTestHat = predict(lasso.fit, newx = X, type='response')\r\n",
    "  n = dim(Y)[1];set.seed(12345)\r\n",
    "  yPred = rnorm(n, yTestHat,stdev)\r\n",
    "  # PLot Actual vs generated data\r\n",
    "  plot(Y, lty=1, col=3)\r\n",
    "  lines(yPred, type='b', col=2)\r\n",
    "```\r\n",
    "  - LASSO:(Best Lambda statistically)   \r\n",
    "      - Checking For Statistically Significantly better: `print(lasso.fit)`\r\n",
    "      - If %Dev explained is similar, choose one with lower 'df'  \r\n",
    "\r\n",
    "    ```r\r\n",
    "    df1 = read.csv2(paste0(file_path, \"Dailytemperature.csv\"))\r\n",
    "    df2 = as.data.frame(df1)\r\n",
    "    temp = df2$Temperature\r\n",
    "    power_factor = seq(-50,50,1)\r\n",
    "    for(i in power_factor){\r\n",
    "      df2[,paste0(\"sin_\",as.character(i))] = sin(0.5^i * temp)\r\n",
    "      df2[,paste0(\"cos_\",as.character(i))] = cos( 0.5^i* temp)\r\n",
    "    }\r\n",
    "    ## LASSO\r\n",
    "    library(glmnet)\r\n",
    "    cols = colnames(df2)[3:204]\r\n",
    "    x = as.matrix(df2[,cols])\r\n",
    "    y = as.matrix(temp)\r\n",
    "    ## Dependence of DF on Penalty factor\r\n",
    "    plot(x = lasso.fit$lambda, y = lasso.fit$df, main=\"Degrees of Freedom V/S  Lambda\",\r\n",
    "        xlab = \"Lambda\", ylab = \"DF\")\r\n",
    "    ## CV for dependence of MSE on Log-Penalty\r\n",
    "    lambdas = seq(-5,5,by = 0.01) \r\n",
    "    lambdas = exp(lambdas)\r\n",
    "    ## Ensure -4 is there in log(lambda)\r\n",
    "    ## any(log(lambdas) == -4)\r\n",
    "    cv.fit = cv.glmnet(x=x,y=y, family=\"gaussian\", type.measure = \"mse\", nfolds = 10,\r\n",
    "                      lambda = lambdas)\r\n",
    "    plot(cv.fit) ## Plots MSE V/S log(lambda)\r\n",
    "    print(cv.fit$lambda.1se) ## Check the Standard Error\r\n",
    "    ## Checking For Statistically Significantly better\r\n",
    "    ## If %Dev explained is similar, choose one with lower 'df'\r\n",
    "    lasso3 = glmnet(x=x,y=y, family = \"gaussian\", alpha=1, lambda = exp(-4))\r\n",
    "    print(lasso3)\r\n",
    "    lasso.min = glmnet(x=x,y=y, family = \"gaussian\", alpha=1, lambda = cv.fit$lambda.min)\r\n",
    "    print(lasso.min)\r\n",
    "    ## Extracting Coefficients\r\n",
    "    coef.exact <- coef(lasso.min,exact = TRUE, x=x, y=y)\r\n",
    "    ## No. of Non Zero Features\r\n",
    "    length(coef.exact[which(coef.exact!=0)]) # Includes Intercept!\r\n",
    "    # Prediction\r\n",
    "    predTemp = predict(lasso.min, newx = x, type = \"link\")\r\n",
    "    predTemp = predict(lasso.min, newx = x, type = \"response\") ## OR\r\n",
    "    ```\r\n",
    "\r\n",
    "<h3 id=\"2-elastic\">Elastic-Net</h3>\r\n",
    "\r\n",
    "  - `alpha` is between `0` and `1`.\r\n",
    "  ```r\r\n",
    "  library(glmnet)\r\n",
    "  Xtrain = train[,-4703]\r\n",
    "  Ytrain = train[,4703]\r\n",
    "  Xtest = test[, -4703]\r\n",
    "  Ytest = test[,4703]\r\n",
    "  grid <- 10 ^ seq (10, -2, length = 100)\r\n",
    "  # Classification Cross-Validation\r\n",
    "  en.fit = cv.glmnet(x = as.matrix(Xtrain),\r\n",
    "                      y = Ytrain, alpha=0.5, lambda = grid, family = 'binomial',\r\n",
    "                    type.measure = 'class')\r\n",
    "  best_lambda = en.fit$lambda.min\r\n",
    "  model = glmnet(x = as.matrix(Xtrain),\r\n",
    "                y = Ytrain, alpha=0.5, lambda = best_lambda, family = 'binomial')\r\n",
    "  yTest.elastic = predict(model, as.matrix(Xtest), type = \"class\")\r\n",
    "  1 - mean(Ytest==yTest.elastic)\r\n",
    "  print(paste0(\"Regularized Features: \", model$df)) # No. of Features\r\n",
    "  ```\r\n",
    "<h3 id=\"2-svm\">SVM</h3>\r\n",
    "\r\n",
    "  - Example also within `Nested Cross-Validation` section \r\n",
    "\r\n",
    "  ```r\r\n",
    "  library(kernlab)\r\n",
    "  form = as.formula(\"Conference~.\")\r\n",
    "  svm.fit = ksvm(form, data = train, kernel = \"vanilladot\")\r\n",
    "  yTest.svm = predict(svm.fit, test)\r\n",
    "  1 - mean(Ytest == yTest.svm)\r\n",
    "  ```\r\n",
    "\r\n",
    "<h3 id=\"2-bag\">Experimental Estimation of Bagging Error</h3>\r\n",
    "\r\n",
    "```r\r\n",
    "D = 10\r\n",
    "mu = as.matrix(runif(D, -10, 10))\r\n",
    "cov_diag = runif(D, 1,2)\r\n",
    "sd_matrix = as.matrix(diag(cov_diag))\r\n",
    "error_data = rmvnorm(100, mean = mu, sigma = sd_matrix)\r\n",
    "error_data = as.matrix(error_data)\r\n",
    "colnames(error_data)=paste0(\"Model \", 1:10)\r\n",
    "## Average Error\r\n",
    "rmse_models = vector(length = 10)\r\n",
    "for(i in 1:10){\r\n",
    "  rmse_models[i] = mean(error_data[,i]^2) ## eq 14.9\r\n",
    "}\r\n",
    "e2 = error_data^2\r\n",
    "boot_data = rowMeans(e2)/10 \r\n",
    "colMeans(e2)%>%mean ## eq 14.10\r\n",
    "mean(boot_data)  ## eq 14.14\r\n",
    "hist(boot_data) # Bagging Error Histogram\r\n",
    "## part-3\r\n",
    "variances = matrix(NA, nrow = 100, ncol=11)\r\n",
    " ## Assuming com(M1,M2) = 0\r\n",
    "for(i in 1:100){\r\n",
    "  # Error Data Generation\r\n",
    "  B = 10\r\n",
    "  mu = as.matrix(runif(B, -10, 10))\r\n",
    "  cov_diag = runif(B, 1,2)\r\n",
    "  sd_matrix = as.matrix(diag(cov_diag))\r\n",
    "  error_data = rmvnorm(100, mean = mu, sigma = sd_matrix)\r\n",
    "  ## Bootstrap Variance\r\n",
    "  error_data = as.matrix(error_data)\r\n",
    "  ## Lecture: 2(a) pg. 10\r\n",
    "  error_bag = rowSums(error_data) / B^2\r\n",
    "  variances[i,1] = mean(error_bag - mean(error_bag)) ## Bagging Error Variance\r\n",
    "  # Model Avg Error Variance\r\n",
    "  variances[i,2:11] = apply(error_data, FUN=var, MARGIN = 2) ## Variance of Average Individual Error\r\n",
    "  \r\n",
    "}\r\n",
    "colnames(variances) = c(\"Bootstrap\", paste0(\"Model \" ,1:10))\r\n",
    "par(mfrow = c(3,4))\r\n",
    "for(i in 1:11){\r\n",
    "  boxplot(variances[,i], main = colnames(variances)[i])\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"2-nn-scratch\">NN from Scratch</h3>  \r\n",
    "\r\n",
    "  - NOTE: X and Y needs to be transposed and also Normalize X!.\r\n",
    "  - Example is for Classification\r\n",
    "  - In case of Regression:\r\n",
    "    - 1. Use `A2 = Z2 # Linear Output` within the `forwardPropagation` function.\r\n",
    "    - 2. Modify the Cost Computing function `computeCost`\r\n",
    "    ```r\r\n",
    "    computeCost = function(X,Y,cache){\r\n",
    "    m = dim(X)[2] # No.of Datapoints\r\n",
    "    A2 = cache$A2 # Output\r\n",
    "    cost <- mean((A2-Y)^2)\r\n",
    "    return (cost)}\r\n",
    "    ```\r\n",
    "```r\r\n",
    "data = read.csv2(paste0(file_path,\"spambase_exam.csv\"))\r\n",
    "X = as.matrix(data[,c(1:57)], ncol = 57)\r\n",
    "X = scale(X)\r\n",
    "Y = as.matrix(data$Spam)\r\n",
    "## Now transpose X and Y: Important Step before Implementation\r\n",
    "X = t(X) \r\n",
    "Y = t(Y)\r\n",
    "# 1. Define Architecture\r\n",
    "# 2. Initialize Model Params\r\n",
    "getLayerSize = function(X,Y,hidden_neurons, train=TRUE){\r\n",
    "  n_x = dim(X)[1]\r\n",
    "  n_h = hidden_neurons\r\n",
    "  n_y = dim(Y)[1]\r\n",
    "  \r\n",
    "  size = list(\r\n",
    "    \"n_x\" = n_x,\r\n",
    "    \"n_h\" = n_h,\r\n",
    "    \"n_y\" = n_y\r\n",
    "  )\r\n",
    "  return(size)\r\n",
    "}\r\n",
    "layers_size = getLayerSize(X,Y,10)\r\n",
    "initializeParameters = function(X, list_layer_size){\r\n",
    "  m = dim(data.matrix(X))[2] ## No of Training Points\r\n",
    "  n_x <- list_layer_size$n_x\r\n",
    "  n_h <- list_layer_size$n_h\r\n",
    "  n_y <- list_layer_size$n_y\r\n",
    "  \r\n",
    "  W1 = matrix(runif(n_h*n_x), nrow = n_h, ncol=n_x, byrow = TRUE)*0.01\r\n",
    "  b1 = matrix(rep(0,n_h), nrow = n_h)\r\n",
    "  \r\n",
    "  W2 = matrix(runif(n_y*n_h), nrow = n_y, ncol = n_h, byrow = TRUE)*0.01\r\n",
    "  b2 <- matrix(rep(0, n_y), nrow = n_y)\r\n",
    "  \r\n",
    "  params <- list(\"W1\" = W1,\r\n",
    "                 \"b1\" = b1, \r\n",
    "                 \"W2\" = W2,\r\n",
    "                 \"b2\" = b2)\r\n",
    "  \r\n",
    "  return (params)\r\n",
    "}\r\n",
    "pars = initializeParameters(X, layers_size)\r\n",
    "## Activations\r\n",
    "sigmoid <- function(x){\r\n",
    "  return(1 / (1 + exp(-x)))\r\n",
    "}\r\n",
    "## Using Tanh in the hidden layer\r\n",
    "## Sigmoid in the Output Layer\r\n",
    "forwardPropagation = function(X, params, list_layer_size){\r\n",
    "  m <- dim(X)[2]\r\n",
    "  n_h <- list_layer_size$n_h\r\n",
    "  n_y <- list_layer_size$n_y\r\n",
    "  W1 <- params$W1\r\n",
    "  b1 <- params$b1\r\n",
    "  W2 <- params$W2\r\n",
    "  b2 <- params$b2\r\n",
    "  b1_new <- matrix(rep(b1, m), nrow = n_h)\r\n",
    "  b2_new <- matrix(rep(b2, m), nrow = n_y)\r\n",
    "  # Input to Hiddem\r\n",
    "  Z1 <- W1 %*% X + b1_new\r\n",
    "  A1 <- tanh(Z1)\r\n",
    "  # A1 <- sigmoid(Z1) # For Sigmoid in input layer\r\n",
    "  # Hidden to Output\r\n",
    "  Z2 <- W2 %*% A1 + b2_new\r\n",
    "  # A2 = Z2 # Linear Output\r\n",
    "  A2 <- sigmoid(Z2) # For Sigmoid\r\n",
    "  cache <- list(\"Z1\" = Z1,\r\n",
    "                \"A1\" = A1, \r\n",
    "                \"Z2\" = Z2,\r\n",
    "                \"A2\" = A2)\r\n",
    "  return (cache)\r\n",
    "}\r\n",
    "fwd_prop <- forwardPropagation(X, pars, layers_size)\r\n",
    "lapply(fwd_prop, function(x) dim(x))\r\n",
    "## Classification\r\n",
    "computeCost = function(X,Y,cache){\r\n",
    "  m = dim(X)[2] # No.of Datapoints\r\n",
    "  A2 = cache$A2 # Output\r\n",
    "  # Binary Loss\r\n",
    "  logprobs = (log(A2) * Y) + (log(1-A2) * (1-Y))\r\n",
    "  cost <- -sum(logprobs/m)\r\n",
    "  return (cost)\r\n",
    "}\r\n",
    "## If Regression\r\n",
    "#computeCost = function(X,Y,cache){\r\n",
    "#  m = dim(X)[2] # No.of Datapoints\r\n",
    "#  A2 = cache$A2 # Output\r\n",
    "#  cost <- mean((A2-Y)^2)\r\n",
    "#  return (cost)\r\n",
    "#}\r\n",
    "cost <- computeCost(X, Y, fwd_prop)\r\n",
    "cost\r\n",
    "backwardPropagation <- function(X, Y, cache, params, list_layer_size){\r\n",
    "  m <- dim(X)[2]\r\n",
    "  n_x <- list_layer_size$n_x\r\n",
    "  n_h <- list_layer_size$n_h\r\n",
    "  n_y <- list_layer_size$n_y\r\n",
    "  A2 <- cache$A2\r\n",
    "  A1 <- cache$A1\r\n",
    "  W2 <- params$W2\r\n",
    "  # Output Layer\r\n",
    "  dZ2 = A2 - Y\r\n",
    "  dW2 = 1/m * (dZ2 %*% t(A1)) \r\n",
    "  db2 <- matrix(1/m * sum(dZ2), nrow = n_y)\r\n",
    "  db2_new <- matrix(rep(db2, m), nrow = n_y)\r\n",
    "  \r\n",
    "  # Input Layer\r\n",
    "  dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2) # If tanh\r\n",
    "  # dZ1 <- (t(W2) %*% dZ2) * (A1*(1*A1)) # If sigmoid\r\n",
    "  dW1 <- 1/m * (dZ1 %*% t(X))\r\n",
    "  db1 <- matrix(1/m * sum(dZ1), nrow = n_h)\r\n",
    "  db1_new <- matrix(rep(db1, m), nrow = n_h)\r\n",
    "  grads <- list(\"dW1\" = dW1, \r\n",
    "                \"db1\" = db1,\r\n",
    "                \"dW2\" = dW2,\r\n",
    "                \"db2\" = db2)\r\n",
    "  \r\n",
    "  return(grads)\r\n",
    "}\r\n",
    "\r\n",
    "back_prop <- backwardPropagation(X, Y, fwd_prop, pars, layers_size)\r\n",
    "lapply(back_prop, function(x) dim(x))\r\n",
    "\r\n",
    "updateParameters = function(grads,params, learning_rate){\r\n",
    "  W1 <- params$W1\r\n",
    "  b1 <- params$b1\r\n",
    "  W2 <- params$W2\r\n",
    "  b2 <- params$b2\r\n",
    "  \r\n",
    "  dW1 <- grads$dW1\r\n",
    "  db1 <- grads$db1\r\n",
    "  dW2 <- grads$dW2\r\n",
    "  db2 <- grads$db2\r\n",
    "  \r\n",
    "  W1 <- W1 - learning_rate * dW1\r\n",
    "  b1 <- b1 - learning_rate * db1\r\n",
    "  W2 <- W2 - learning_rate * dW2\r\n",
    "  b2 <- b2 - learning_rate * db2\r\n",
    "  \r\n",
    "  updated_params <- list(\"W1\" = W1,\r\n",
    "                         \"b1\" = b1,\r\n",
    "                         \"W2\" = W2,\r\n",
    "                         \"b2\" = b2)\r\n",
    "  \r\n",
    "  return (updated_params)\r\n",
    "}\r\n",
    "update_params <- updateParameters(back_prop, pars, learning_rate = 0.01)\r\n",
    "lapply(update_params, function(x) dim(x))\r\n",
    "\r\n",
    "## Training\r\n",
    "trainModel <- function(X, y, num_iteration, hidden_neurons, lr){\r\n",
    " # X = t(X) # Transpose if not done already.\r\n",
    " # y = t(y)\r\n",
    "  layer_size <- getLayerSize(X, y, hidden_neurons)\r\n",
    "  init_params <- initializeParameters(X, layer_size)\r\n",
    "  cost_history <- c()\r\n",
    "  for (i in 1:num_iteration) {\r\n",
    "    fwd_prop <- forwardPropagation(X, init_params, layer_size)\r\n",
    "    cost <- computeCost(X, y, fwd_prop)\r\n",
    "    back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)\r\n",
    "    update_params <- updateParameters(back_prop, init_params, learning_rate = lr)\r\n",
    "    init_params <- update_params\r\n",
    "    cost_history <- c(cost_history, cost)\r\n",
    "    \r\n",
    "    if (i %% 50 == 0) cat(\"Iteration\", i, \" | Cost: \", cost, \"\\n\")\r\n",
    "  }\r\n",
    "  \r\n",
    "  model_out <- list(\"updated_params\" = update_params,\r\n",
    "                    \"cost_hist\" = cost_history)\r\n",
    "  return (model_out)\r\n",
    "}\r\n",
    "## Learning\r\n",
    "EPOCHS = 500\r\n",
    "HIDDEN_NEURONS = 10\r\n",
    "LEARNING_RATE = 0.1\r\n",
    "train_model <- trainModel(X, Y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)\r\n",
    "# Making Prediction\r\n",
    "makePrediction <- function(X, y, hidden_neurons){\r\n",
    "   # X = t(X) # Transpose if doing in 'trainModel'.\r\n",
    "  # y = t(y)\r\n",
    "  layer_size <- getLayerSize(X, y, hidden_neurons)\r\n",
    "  params <- train_model$updated_params\r\n",
    "  fwd_prop <- forwardPropagation(X, params, layer_size)\r\n",
    "  pred <- fwd_prop$A2\r\n",
    "  return (pred)\r\n",
    "}\r\n",
    "outputProb = makePrediction(X,Y,HIDDEN_NEURONS)\r\n",
    "```\r\n",
    "<h3 id=\"nsc\">Nearest Shrunken Centroid</h3>\r\n",
    "\r\n",
    "  - Need to scale the `x` data\r\n",
    "  - Class Probability Estimate $\\hat{p}_k(x^{*}) =$ $\\frac{\\mathrm{e}^{\\delta_k (x^*)}}{\\sum_{l=1}^{K} \\mathrm{e}^{\\delta_l (x^*)}}$\r\n",
    "  ```r\r\n",
    "  # To Obtain Class Probability estimate\r\n",
    "  pamr.predict(model, newx, threshold,type =  \"posterior\",prior = model$prior, threshold.scale = model$threshold.scale)\r\n",
    "  ```\r\n",
    "```r\r\n",
    "  library(pamr)\r\n",
    "  data = (read.csv2(paste0(file_path, \"data.csv\")) )\r\n",
    "  data$Conference = as.factor(data$Conference) # Categorical as factor\r\n",
    "  # Give rows name\r\n",
    "  # Train/Test Split\r\n",
    "  train = data[id,]\r\n",
    "  test = data[-id,]\r\n",
    "  # Transpose the X matrix!!!\r\n",
    "  x = t(train[,c(1:4702)]) \r\n",
    "  y = train[['Conference']]\r\n",
    "  # Modify Data\r\n",
    "  mydata = list(x=x,y=as.factor(y),geneid= as.character(1:nrow(x)), genenames= rownames(x))\r\n",
    "  model = pamr.train(mydata, threshold = seq(0,9,0.1))\r\n",
    "  # Cross Validation\r\n",
    "  #### SET SEEED!!!!!\r\n",
    "  set.seed(12345)\r\n",
    "  cv.results = pamr.cv(model, data = mydata, nfold = 5)\r\n",
    "  # Plot Cross Validation Results\r\n",
    "  # THreshold for which error is minimum and parameters are less\r\n",
    "  pamr.plotcv(cv.results)\r\n",
    "  # Compute Confusion matrix for a threshold\r\n",
    "  pamr.confusion(model, threshold = 1.9)\r\n",
    "  # Plot cross validated class probabilities by class\r\n",
    "  pamr.plotcvprob(model,data=mydata, threshold = 1.9)\r\n",
    "  # Estimate False discovery rates and plot them\r\n",
    "  fdr.obj = pamr.fdr(model, mydata)\r\n",
    "  plot(x= fdr.obj$results[,1], y = fdr.obj$results[,4], xlab=\"Threshold\",\r\n",
    "      ylab=\"False Discovery Rate\")\r\n",
    "  plot(x= fdr.obj$results[,2], y = fdr.obj$results[,4], xlab=\"Significant Genes\",\r\n",
    "      ylab=\"False Discovery Rate\")\r\n",
    "  # List Significant Genes: ALso find their names\r\n",
    "  res = pamr.listgenes(model, mydata,threshold = 1.9)\r\n",
    "  names = as.integer(res[1:10,1])\r\n",
    "  colnames(train)[names] # Get Column Names\r\n",
    "  # Plot Centroids\r\n",
    "  pamr.plotcen(model,mydata, threshold = 1.9)\r\n",
    "  # Prediction\r\n",
    "  x = t(test[,c(1:4702)]) \r\n",
    "  y = test[['Conference']]\r\n",
    "  mydata2 = list(x=x,y=as.factor(y),geneid= as.character(1:nrow(x)), genenames= rownames(x))\r\n",
    "  yTestHat = pamr.predict(model, mydata2$x, threshold=1.9)\r\n",
    "  # Class Posterior Probability\r\n",
    "  yTest.Prob = pamr.predict(model, mydata2$x, threshold=1.9, type=\"posterior\")\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"pca\">PCA</h3>\r\n",
    "\r\n",
    "  - For PC1: $eigen_{1\\lambda_1}$*column-1 + $eigen_{2\\lambda_1}$*column-2 +...\r\n",
    "    - r-equivalent: `PC1 = x2%*%as.matrix(eig.x$vectors[,1])` where $x2$ is NXD matrix and `eig.x$vector[,1]` is a `DX1` eigen-vectors for $\\lambda_1$      \r\n",
    "\r\n",
    "```r\r\n",
    "  ### Manual PCA\r\n",
    "  df = mtcars\r\n",
    "  X = df[, c(\"mpg\", \"hp\")]\r\n",
    "  x2 = scale(X) # Standardizing\r\n",
    "  x3 = x2 - colMeans(x2) ## Subrtracting Means\r\n",
    "  cov.x = cov(x3)\r\n",
    "  eig.x = eigen(cov.x)\r\n",
    "  eig.x$vectors\r\n",
    "  ## Ensure that x and y are standardized\r\n",
    "  plot(x = x2[,1], y = x2[,2], cex=2, xlab=\"x-component\",ylab=\"y component\",\r\n",
    "      main=\"Actual Data VS PCA Vectors\")\r\n",
    "  ## Draw PC1\r\n",
    "  arrows(x0=0,y0=0,x1 = eig.x$vectors[1,1], y1=eig.x$vectors[2,1], col=\"red\", lwd = 5)\r\n",
    "  arrows(x0=0,y0=0,x1 = -eig.x$vectors[1,1], y1=-eig.x$vectors[2,1], col=\"red\", lwd = 5)\r\n",
    "  ## Draw PC2\r\n",
    "  arrows(x0=0,y0=0,x1 = eig.x$vectors[1,2], y1=eig.x$vectors[2,2], col=\"blue\", lwd = 5)\r\n",
    "  arrows(x0=0,y0=0,x1 = -eig.x$vectors[1,2], y1=-eig.x$vectors[2,2], col=\"blue\", lwd = 5)\r\n",
    "  ## Reduced Data V/S PCA\r\n",
    "  PC1 = x2%*%as.matrix(eig.x$vectors[,1])\r\n",
    "  PC2 = x2%*%as.matrix(eig.x$vectors[,2])\r\n",
    "  # Reduced Data\r\n",
    "  ## Here PC's need to be rotated\r\n",
    "  plot(x = PC1, y = PC2, cex=2, xlab=\"PC1\",ylab=\"PC2\",\r\n",
    "      main=\"Data Along PC1 and PC2\", xlim = c(-3,3), ylim = c(-3,3))\r\n",
    "  arrows(x0=0,y0=0,x1 = -3, y1=0, col=\"red\", lwd = 5)\r\n",
    "  arrows(x0=0,y0=0,x1 = 3, y1=0, col=\"red\", lwd = 5)\r\n",
    "  arrows(x0=0,y0=0,x1 =0, y1=2, col=\"blue\", lwd = 5)\r\n",
    "  arrows(x0=0,y0=0,x1 = 0, y1=-2, col=\"blue\", lwd = 5)\r\n",
    "\r\n",
    "  ##### Communities data example\r\n",
    "  communities = read.csv(paste0(file_path, 'communities.csv'))\r\n",
    "  covs = colnames(communities)[1:100]\r\n",
    "  # Manual PCA\r\n",
    "  # Step-1: Scaling\r\n",
    "  PCA  = scale(communities[,covs])\r\n",
    "  PCA = cbind(PCA, communities$ViolentCrimesPerPop)\r\n",
    "  cor.mat = cor(PCA) # Correlation Matrix\r\n",
    "  cov.mat = cov(PCA) # Covariance Matrix\r\n",
    "  # Step-2: Obtain  eigenvalues and eigenvectors of covariance matrix\r\n",
    "  eig = eigen(cov.mat) # Eigen values and vectors\r\n",
    "  # Step-3: Calculate Total and Proportional Variance explained\r\n",
    "  lambdas = eig$values\r\n",
    "  # First 34 will explain 95% Variance in the data\r\n",
    "  cum_var = cumsum(lambdas/sum(lambdas))\r\n",
    "  sum(cum_var<0.95)\r\n",
    "  # Proportion of variance explained by 1st two Principal Components\r\n",
    "  lambdas[1:2]/sum(lambdas)\r\n",
    "  pc.result = princomp(PCA)\r\n",
    "  # pc.result$centre: Specifies 'Mean' of the variables used for PCA\r\n",
    "  # pc.result$sdev: Specifies 'Standard Deviation' of the variables used for PCA\r\n",
    "  # Calculate Total and Proportional Variance explained\r\n",
    "  pc.var = pc.result$sdev^2\r\n",
    "  pve = pc.var/sum(pc.var)\r\n",
    "  # Plot 1st 2 Principal Components\r\n",
    "  library(ggplot2)\r\n",
    "  PC1 = pc.result$scores[,1]\r\n",
    "  PC2 = pc.result$scores[,2]\r\n",
    "  PCA2 = data.frame(VioLentCrimesPerPop = communities$ViolentCrimesPerPop, PC1,PC2)\r\n",
    "  ggplot(PCA2, aes(PC1,PC2, colour = VioLentCrimesPerPop))+\r\n",
    "          geom_point() + theme_bw()\r\n",
    "  # Second Order Polynomial on PC1\r\n",
    "  X = poly(PC1,2, raw=TRUE)  # MAKE SURE 'raw=TRUE'      \r\n",
    "  Y = communities$ViolentCrimesPerPop\r\n",
    "  data = data.frame(cbind(X,Y))\r\n",
    "  # Prediction\r\n",
    "  form = as.formula(\"Y~.\")\r\n",
    "  fit = lm(form, data=data)\r\n",
    "  Yhat = predict(fit, newdata = data)\r\n",
    "  plot(x = data$X1, y = data$Y,\r\n",
    "      xlab = \"PC-1\", ylab = 'Violent Crimes', cex=0.5)\r\n",
    "  points(x = data$X1, y = Yhat,col='red', cex=0.5)\r\n",
    "  legend(\"topleft\", pch = 1, col = c(\"black\", \"red\"), legend = c(\"Actual\", \"Predicted\"),\r\n",
    "        cex=0.5)\r\n",
    "  # Run Parametric Bootstrap\r\n",
    "  result = boot(data, statistic = f1, R=1000, mle = mle,ran.gen = rng,sim = 'parametric')\r\n",
    "  result.PI = boot(data, statistic = f2, R=1000, mle = mle,ran.gen = rng,sim = 'parametric')\r\n",
    "  # Plot Confidence Interval\r\n",
    "  env = envelope(result, level=0.95)\r\n",
    "  env.PI = envelope(result.PI,level=0.95)\r\n",
    "  plot(x = data$X1, y = data$Y, ylim=c(-1,2),\r\n",
    "      xlab = \"PC-1\", ylab = 'Violent Crimes', cex=0.5)\r\n",
    "  points(x = data$X1, y = Yhat,col=2, cex=0.5,  lty=1)\r\n",
    "  points(x = data$X1, y = env$point[2,], col = 3)\r\n",
    "  points(x = data$X1, y = env$point[1,], col = 3)\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"npb\">Non-Parametric Bootstrap</h3>\r\n",
    "\r\n",
    "  - Assumes no distribution for the generated data.\r\n",
    "  - **USE the WHOLE TRAINING DATA** \r\n",
    "\r\n",
    "```r\r\n",
    "library(boot)\r\n",
    "state = read.csv(paste0(file_path,\"State.csv\"), header = TRUE,sep = \";\")\r\n",
    "to_numeric = colnames(state)[2:6]\r\n",
    "for(col in to_numeric){\r\n",
    "  state[,col] = as.numeric(gsub(\",\",\".\", state[,col])) # String modification\r\n",
    "}\r\n",
    "# 1. Re-order the Data based on co-variates\r\n",
    "  state.order = state[order(state$MET),] # Ascending Order\r\n",
    "# 2.1 Define function to 'fit' and 'predict' on the sampled data\r\n",
    "  f = function(data,index){\r\n",
    "    data1 = data[index,]\r\n",
    "    fit = tree(form, data1,control = tree.control(nobs= dim(state)[1],minsize=8))\r\n",
    "    prunedTree  = prune.tree(fit, best=3)\r\n",
    "    Yhat = predict(prunedTree, newdata = state.order)\r\n",
    "    return(Yhat)\r\n",
    "  }\r\n",
    "# 2.2 For Prediction Intervals\r\n",
    "  f2 = function(data,index){\r\n",
    "    data1 = data[index,]\r\n",
    "    fit = tree(form, data1,control = tree.control(nobs= dim(state)[1],minsize=8))\r\n",
    "    prunedTree  = prune.tree(fit, best=3)\r\n",
    "    Yhat = predict(prunedTree, newdata = state.order)\r\n",
    "    n = length(state$EX) # equal to #.Training Data points\r\n",
    "    # Now generate Y from the distribution\r\n",
    "    predictedEX = rnorm(n, Yhat, sd(residuals(prunedTree)))\r\n",
    "    return(predictedEX) # Return this]\r\n",
    "  }\r\n",
    "# 3. Perform Non-Parametric Bootstrap\r\n",
    "  result = boot(state.order, f, R=1000) # Confidence Interval\r\n",
    "  result.PI = boot(state.order, f2, R=1000) # Prediction Interval\r\n",
    "# 4. Plot Confidence Intervals\r\n",
    "  env = envelope(result,level=0.95)# For Setting the CI\r\n",
    "  env.PI = envelope(result.PI, level=0.95)\r\n",
    "  # Prediction with CI\r\n",
    "# Prediction with CI\r\n",
    "par(mfrow=c(1,1))\r\n",
    "plot(x = state.order$MET, y = state.order$EX, col = \"red\")\r\n",
    "points(x = state.order$MET, y = trY, col = \"blue\",type='l')\r\n",
    "points(x = state.order$MET, y = env$point[2,], col = \"blue\",type='l')\r\n",
    "points(x = state.order$MET, y = env$point[1,], col = \"blue\",type='l')\r\n",
    "points(x = state.order$MET, y = env.PI$point[2,], col = \"green\",type='l')\r\n",
    "points(x = state.order$MET, y = env.PI$point[1,], col = \"green\",type='l')\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"pb-r\">Parametric Bootstrap(Regression)</h3>  \r\n",
    "\r\n",
    "  - Need to modify `rng` depending upon classification or regression task!\r\n",
    "  - For Classification look into: `Parametric Bootstrap(Classification)`\r\n",
    "\r\n",
    "```r\r\n",
    "library(boot)\r\n",
    "state = read.csv(paste0(file_path,\"State.csv\"), header = TRUE,sep = \";\")\r\n",
    "to_numeric = colnames(state)[2:6]\r\n",
    "for(col in to_numeric){\r\n",
    "  state[,col] = as.numeric(gsub(\",\",\".\", state[,col])) # String modification\r\n",
    "}\r\n",
    "# 1. Compute 'mle' i.e., of model type\r\n",
    "mle = tree(form, state,control = tree.control(nobs= dim(state)[1],minsize=8))\r\n",
    "residuals(mle) # Best way to obtain model residuals\r\n",
    "# 2. Define 'rng'  for data generation\r\n",
    "rng = function(data, mle){\r\n",
    "  data1 = data.frame(EX = data$EX,MET= data$MET)\r\n",
    "  n = length(data$EX) # For Whole Data\r\n",
    "  # Generate New Data: rnorm will be changed for classification task\r\n",
    "  data1$EX = rnorm(n, predict(mle, newdata = data1), sd(residuals(mle)))\r\n",
    "  return(data1)\r\n",
    "}\r\n",
    "#  Statistic function: should return the estimator(Yhat)\r\n",
    "f1 = function(data1){\r\n",
    "  res = tree(form, data1,control = tree.control(nobs= dim(state)[1],minsize=8)) # Fit the Model\r\n",
    "  Yhat = predict(res, newdata = state) # newdata = input data\r\n",
    "  return(Yhat)\r\n",
    "}\r\n",
    "# Run Parametric Bootstrap\r\n",
    "result = boot(state, statistic = f1, R=1000, mle = mle,ran.gen = rng,sim = 'parametric')\r\n",
    "## Prediction Interval\r\n",
    "f2 = function(data1){\r\n",
    "  # Fit the Model\r\n",
    "  res = tree(form, data1,control = tree.control(nobs= dim(state)[1],minsize=8))\r\n",
    "  Yhat = predict(res, newdata = state) # newdata = input data\r\n",
    "  n = length(state$EX) # equal to no of Training Data points\r\n",
    "  # Now generate Y from the distribution\r\n",
    "  predictedEX = rnorm(n, Yhat, sd(residuals(mle)))\r\n",
    "  return(predictedEX) # Return this\r\n",
    "}\r\n",
    "# Plot Confidence Interval\r\n",
    "env = envelope(result, level=0.95)\r\n",
    "result.PI = boot(state, statistic = f2, R=1000, mle = mle,\r\n",
    "                 ran.gen = rng,sim = 'parametric')\r\n",
    "env.PI = envelope(result.PI,level=0.95)\r\n",
    "# Prediction with CI& PI\r\n",
    "plot(x = state.order$MET, y = state.order$EX, col = \"red\",\r\n",
    "     ylim = c(min(c(env.PI$point[2,],env.PI$point[1,])),max(c(env.PI$point[2,],env.PI$point[1,]))))    \r\n",
    "points(x = state.order$MET, y = trY, col = \"blue\",type='l')\r\n",
    "points(x = state.order$MET, y = env$point[2,], col = \"red\",type='l')\r\n",
    "points(x = state.order$MET, y = env$point[1,], col = \"red\",type='l')\r\n",
    "points(x = state.order$MET, y = env.PI$point[2,], col = \"green\",type='l') # For Prediction Interval\r\n",
    "points(x = state.order$MET, y = env.PI$point[1,], col = \"green\",type='l') # For Prediction Interval\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"pb-c\">Parametric Bootstrap(Classification)</h3> \r\n",
    "\r\n",
    "  - Pay attention to `rng` function \r\n",
    "\r\n",
    "   ```r\r\n",
    "    # Perform Parametric Bootstrap\r\n",
    "    # Since it is a classification task: 'rng' and f1 need to be modified\r\n",
    "    data = data.frame(PC1 = PC_one,Y = digits$Y)\r\n",
    "    data$Y = as.factor(data$Y)\r\n",
    "    form = as.formula(\"Y~.\")\r\n",
    "    # 1. Define 'mle'\r\n",
    "    mle = glm(form, data = data, family='binomial')\r\n",
    "    # 2. Define 'rng'  for data generation\r\n",
    "    rng = function(data, mle){\r\n",
    "    data1 = data.frame(PC1 = data[,1], Y= data[,2])\r\n",
    "    n = length(data[,2]) # For Whole Data\r\n",
    "    # Generate New Data: rbinom for classification task\r\n",
    "    data1[,2] = rbinom(n, size = 1, prob = predict(mle, data1, type = 'response'))\r\n",
    "    return(data1)\r\n",
    "    }\r\n",
    "    #  3. Statistic function: should return the estimator(Yhat)\r\n",
    "    f1 = function(data1){\r\n",
    "    # Fit the Model\r\n",
    "    fit = glm(form, data = data1, family='binomial')\r\n",
    "    # newdata = input data\r\n",
    "    Yhat = predict(fit, newdata = data) # Get the Link\r\n",
    "    # Since Predicting Confidence Interval of \"Probabilities\", it cannot be negative(ISLR pg.291-292)\r\n",
    "    Yhat = exp(Yhat)/(1 + exp(Yhat))\r\n",
    "    return(Yhat)\r\n",
    "    }\r\n",
    "    # Run Parametric Bootstrap\r\n",
    "    result = boot(data, statistic = f1, R=1000, mle = mle,ran.gen = rng,sim = 'parametric')\r\n",
    "    env = envelope(result,level=0.95)# For Setting the CI\r\n",
    "    # Plot results\r\n",
    "    plot(x = data$PC1, y = yT, col = \"red\", ylim=c(0,1))\r\n",
    "    points(x = data$PC1, y = env$point[2,], col = \"blue\")\r\n",
    "    points(x = data$PC1, y = env$point[1,], col = \"blue\")\r\n",
    "  ```\r\n",
    "\r\n",
    "<h3 id=\"bonferroni\">Bonferroni Method</h3>   \r\n",
    "\r\n",
    "  - When we perform several Tests simultaneously, we need to make an adjustment for risk.\r\n",
    "  - $H_{0j}$ :  Treatment has no effect on $j$(j will be column or row)\r\n",
    "  - $H_{1j}$ :  Treatment has an effect on $j$\r\n",
    "  - We reject $H_{0j}$ if $p_j < \\alpha/M$. Not useful for large M for which Bonferroni gives conservative results\r\n",
    "  - Choose features for which we fail to reject the Null Hypothesis!  \r\n",
    "\r\n",
    "<h3 id=\"b-h\">Benjamin-Hochberg</h3>  \r\n",
    "\r\n",
    "  - $H_{0j}$ :  treatment has no effect on $j$(j will be column or row) \r\n",
    "  - $H_{1j}$ :  Treatment has an effect on $j$    \r\n",
    "  - ISLR(Version 2) has example on Regression type Data. Here we deal with Classification Data:   \r\n",
    "    - In CLassification Data, divide the data for each class and conduct BH Test for each class individually.     \r\n",
    "\r\n",
    "```r\r\n",
    "  gene_data = read.csv(paste0(file_path,\"geneexp.csv\"))\r\n",
    "  gene_data$CellType = as.factor(gene_data$CellType)\r\n",
    "  which(sapply(gene_data, class) !='integer') # Check column-type\r\n",
    "  expnames = colnames(gene_data)[2:2086] # X-column names\r\n",
    "  gene_data[['isCD4']] = gene_data$CellType == \"CD4\" # Conditional Column for CD4 Cell type\r\n",
    "  gene_data$isCD8 = gene_data$CellType == \"CD8\"\r\n",
    "  gene_data$isCD19 = gene_data$CellType == \"CD19\"\r\n",
    "  ## Benjamin-Hochberg\r\n",
    "  # 1. Specify 1, the level to which control FDR\r\n",
    "  q = 0.05\r\n",
    "  # 2. Compute p values for m-null hypothesis\r\n",
    "  get_p_value = function(name,gene_type, data){\r\n",
    "    f = as.formula(paste0(name,\"~\",gene_type)) #\r\n",
    "    return(t.test(f,data = data)$p.value)\r\n",
    "  }\r\n",
    "  ## CD4\r\n",
    "  p.CD4 = sapply(expnames, FUN=get_p_value, gene_type = \"isCD4\", data = gene_data)\r\n",
    "  ## CD8\r\n",
    "  p.CD8 = sapply(expnames, FUN=get_p_value, gene_type = \"isCD8\", data = gene_data)\r\n",
    "  ## CD19\r\n",
    "  p.CD19 = sapply(expnames, FUN=get_p_value, gene_type = \"isCD19\", data = gene_data)\r\n",
    "  # 3. Order m p-values!!!!!\r\n",
    "  p.CD4 = sort(p.CD4) # IMPORTANT step\r\n",
    "  p.CD8 = sort(p.CD8)\r\n",
    "  p.CD19 = sort(p.CD19)\r\n",
    "  # 4. Define L\r\n",
    "  BH = function(pv, q){\r\n",
    "    m = length(pv)\r\n",
    "    for(i in 1:m){\r\n",
    "      if(pv[i]>q*i/m)\r\n",
    "        break\r\n",
    "    }\r\n",
    "    return(i-1)\r\n",
    "  }\r\n",
    "  # 5. Reject all null hypothesis for which p_j <= p_{(L)}\r\n",
    "  BH(p.CD4, q=0.05) ## No of Rejected Genes for CD4 Cell type(Choose these as important ones)\r\n",
    "  # Important Ones\r\n",
    "  names(p.CD4)[1:BH(p.CD4, q=0.05)] # These are the important ones\r\n",
    "  BH(p.CD8, q=0.05) ## No of Rejected Genes for CD8 Cell type\r\n",
    "  BH(p.CD19, q=0.05) ## No of Rejected Genes for CD19 Cell type\r\n",
    "  # Plotting\r\n",
    "  BHLine = q*(1:length(p.CD4))/m # Line for BH-threshold\r\n",
    "  plot(x=1:length(p.CD4), y=p.CD4, xlab='Index',\r\n",
    "      ylab=\"P-Value\", main = \"CD4\")\r\n",
    "  lines(x=1:length(p.CD4),y = BHLine, col=\"blue\", type='l')\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"n-cv\">N-Fold CV</h3>  \r\n",
    "\r\n",
    "  1. Calculate Loss using `loss`\r\n",
    "  2. Calculate N-Fold Loss for the data using 'n_fold_cv'\r\n",
    "  3. 'get.subset' creates various column subsets. NOTE: Does not create for a single predictor.\r\n",
    "\r\n",
    "```r\r\n",
    "loss = function(x,y,xTest,yTest){\r\n",
    "  # Function to obtain regression coefficient using x,y\r\n",
    "  # Predict y_hat for 'xTest' and then calculate loss\r\n",
    "  x = as.matrix(cbind(intercept=1,x)) ### Add Intercept\r\n",
    "  xTest = as.matrix(cbind(intercept=1,xTest))\r\n",
    "  xtx = solve(t(x)%*%x)\r\n",
    "  beta = xtx%*%(t(x)%*%y)\r\n",
    "  beta = as.matrix(t(beta))\r\n",
    "  colnames(beta) = colnames(x)\r\n",
    "  # loss =  mean(abs(y - y_hat))  ## MAE\r\n",
    "  yHat = xTest%*%t(beta)\r\n",
    "  testLoss = sqrt(mean((yTest-yHat)**2)) \r\n",
    "  return(testLoss)\r\n",
    "}\r\n",
    "n_fold_cv = function(X,Y, n_fold){\r\n",
    "  #Randomly shuffle the data\r\n",
    "  ran_ind = sample(nrow(X))\r\n",
    "  X = as.matrix(X[ran_ind,])\r\n",
    "  Y = Y[ran_ind]\r\n",
    "  #Create n equally size folds\r\n",
    "  folds <- cut(seq(1,nrow(X)),breaks=n_fold,labels=FALSE)\r\n",
    "  #Perform 10 fold cross validation\r\n",
    "  cv_loss = c()\r\n",
    "  for(i in 1:n_fold){\r\n",
    "    #Segement your data by fold using the which() function \r\n",
    "    testIndexes <- which(folds==i,arr.ind=TRUE)\r\n",
    "    xTest <- X[testIndexes,]\r\n",
    "    yTest <- Y[testIndexes]\r\n",
    "    xTrain <- X[-testIndexes,]\r\n",
    "    yTrain <- Y[-testIndexes]\r\n",
    "    l = loss(x=xTrain, y = yTrain,xTest = xTest, yTest = yTest)\r\n",
    "    cv_loss = c(l,cv_loss)\r\n",
    "  }\r\n",
    "  cv_loss = mean(cv_loss)\r\n",
    "  return(cv_loss)\r\n",
    "}\r\n",
    "get.subset = function(X,Y,n_folds){\r\n",
    "  N = dim(X)[2] # No. of Co-variates\r\n",
    "  \r\n",
    "  col_comb = function(i,n){t(combn(1:n,i))} # Function to select 'i' out of 'n'\r\n",
    "  best_cols = col_comb(N,N)[1,] # Start with all columns\r\n",
    "  loss_best = Inf  # Setting a very high initial value\r\n",
    "  \r\n",
    "  # i accounts for no of columns\r\n",
    "  for(i in 1:N){\r\n",
    "    combos = col_comb(i=i,n=N)\r\n",
    "    rows = dim(combos)[1]\r\n",
    "    for(row in 1:rows){\r\n",
    "      columns = combos[row,]\r\n",
    "      X_new = as.matrix(X[, columns])\r\n",
    "      # HERE n-fold validation\r\n",
    "      curr_loss = n_fold_cv(X_new, Y, n_fold)\r\n",
    "      if (curr_loss<loss_best){\r\n",
    "        loss_best = curr_loss\r\n",
    "        best_cols = columns\r\n",
    "      }\r\n",
    "    }\r\n",
    "  }\r\n",
    "  return(list(best = best_cols, loss =loss_best))\r\n",
    "}\r\n",
    "```\r\n",
    "<h3 id=\"step-aic\">Variable Selection using StepAIC</h3>  \r\n",
    "\r\n",
    "  1. Can be used for both `aic` and `bic`\r\n",
    "  2. What are the important variables, that can be asked\r\n",
    "  3. 'AIC' is an information criterion and can use all the provided data. Thus, we can utilize all the provided data as information for estimating generalization capabilities of a model.\r\n",
    "\r\n",
    "```r\r\n",
    "  ## Variable Selection using StepAIC\r\n",
    "  library(MASS)\r\n",
    "  d3 = data[,-1] # Sample Dataset\r\n",
    "  form = \"Fat~.\" # Set Formula\r\n",
    "  fit = lm(form, data = as.data.frame(d3))\r\n",
    "  step = stepAIC(fit, direction = 'both') # For AIC in both the directions.\r\n",
    "  step.BIC = stepAIC(fit, k=log(nrow(d3))) # BIC Method\r\n",
    "  # To get selected variables\r\n",
    "  get.coef.names = function(model, step.func){\r\n",
    "    STEP_COEF = vector(\"numeric\",length(coefficients(model)))# create an empty vector of zeros\r\n",
    "    names(STEP_COEF) = names(coefficients(model))#same names\r\n",
    "    STEP_COEF[names(coefficients(step))] = as.numeric(coefficients(step))#fill in the ones found in step\r\n",
    "    coef_names = names(STEP_COEF[round(STEP_COEF, 0) !=0])\r\n",
    "    return(coef_names)\r\n",
    "  }\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"nested-cv\">Nested Cross-Validation</h3>   \r\n",
    "\r\n",
    "  - randomly split data in $K$ `outerfolds`\r\n",
    "  - for each hyper-parameter combo, there will be an avegare combo. error `config_error` calculated using only `Val` data within inner-loop.\r\n",
    "  - The inner-loop consists of inner-CV for `innerfolds` times cross-validation.\r\n",
    "  - Look at the average evaluation for each config i.e., `config_error` and choose the hyper-parameter with lowest `config_error`\r\n",
    "  - Now, with the chosen hyper-parameters, divide the data into `Train` and `Test` for each fold and save the test loss within `outer_cv_loss` for each outer fold.\r\n",
    "  - The average of the `outer_cv_loss` is the models `Generalization` Error.  \r\n",
    "\r\n",
    "```r\r\n",
    "data(spam)\r\n",
    "outerfolds = 5 # No. of Outer Folds\r\n",
    "innerfolds = 3 # No. of Inner FOlds\r\n",
    "form = as.formula('type~.')\r\n",
    "ran_ind = sample(nrow(spam)) # Reshuffle the Data\r\n",
    "data = spam[ran_ind,]\r\n",
    "#Create n equally size folds depending on 'outerfolds' value\r\n",
    "folds <- cut(seq(1,nrow(X)),breaks=outerfolds,labels=FALSE)\r\n",
    "#Perform n fold cross validation\r\n",
    "outer_cv_loss = NULL # Each Models Error\r\n",
    "params = seq(0.1,2.5,0.5)\r\n",
    "# Outer CV\r\n",
    "for(outerfold in 1:outerfolds){\r\n",
    "    testIndexes <- which(folds==outerfold,arr.ind=TRUE)\r\n",
    "    Test <- data[testIndexes,]\r\n",
    "    Train <- data[-testIndexes,]\r\n",
    "    # Hyper-Parameter Search: Use only Validation Data Error\r\n",
    "    config_error = NULL\r\n",
    "     # Each 'i' should contain the required param(s) combo\r\n",
    "      for(i in 1:length(params)){\r\n",
    "        # Inner-CV\r\n",
    "        if_error = NULL # Inner-Fold error for 'i' Param configuration\r\n",
    "        for(fold in 1:innerfolds){\r\n",
    "          #   # Use only xTrain\r\n",
    "            ValIndexes <- which(folds==fold,arr.ind=TRUE)\r\n",
    "            Val <- Train[ValIndexes,] # kth 'inner-fold' Validation Data\r\n",
    "            TraIN <- Train[-ValIndexes,]\r\n",
    "            fit = ksvm(form,data=TraIN,kernel=\"rbfdot\",kpar=list(sigma=0.05),C=params[i])\r\n",
    "            yValHat = predict(fit, Val)\r\n",
    "            misc_error = 1-mean(Val$type == yValHat) # Misclassification Error\r\n",
    "            if_error = c(if_error, misc_error)\r\n",
    "        }\r\n",
    "        config_error[i] = mean(if_error) # Look at avg eval for each config\r\n",
    "      }\r\n",
    "    best_i = params[which.min(config_error)] # Choose the config with the best config_error\r\n",
    "    fit.outer = ksvm(form,data=Train,kernel=\"rbfdot\",kpar=list(sigma=0.05),C=best_i)\r\n",
    "    yTestHat = predict(fit.outer, Test) # Prediction on k-th outerfold Test Data\r\n",
    "    of_error = 1-mean(Test$type == yTestHat) # Outer Fold Test Error\r\n",
    "    outer_cv_loss = c(outer_cv_loss, of_error) # Its Mean is the 'Generalized Error'\r\n",
    "}\r\n",
    "generalization_error = mean(outer_cv_loss) # Generalization error of the model.\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"laplace\">Laplace Prior</h3>   \r\n",
    "\r\n",
    "  - $Y_i = \\mathbf{x}_i + \\mathbf{\\beta} + \\epsilon_i, \\text{i = 1,...,n}$ where $\\epsilon_i \\sim \\textit{Laplace(0,b)}$ then, the complete data Log-Likelihood is given as,\r\n",
    "    - $ - \\ln p\\left( Y| \\mathbf{X}, \\mathbf{\\beta}, b \\right) = n \\ln (\\textit{2b}) + \\frac{1}{b} \\sum_{i=1}^n \\left|\\mathbf{x}_i \\cdot \\mathbf{\\beta} - Y_i \\right|$\r\n",
    "  - Creating distribution for error term $\\epsilon$ where $p(\\epsilon) = \\frac{e^{-|\\epsilon|}}{2}$\r\n",
    "  - `e_s` draws from the Laplacian Error distribution with $\\mu = \\mathbf{0}$ and $\\Sigma = \\mathbf{1}$\r\n",
    "```r\r\n",
    "# Drawing Error from its Laplace distribution\r\n",
    "error = seq(-5,5, length = 10000)\r\n",
    "dlaplace = function(x){0.5*exp(-abs(x))}\r\n",
    "derror = sapply(error, FUN = dlaplace)\r\n",
    "e_s = sample(error, size=25000,prob = derror, replace=TRUE)\r\n",
    "hist(e_s, breaks = 1000, freq = F)\r\n",
    "```\r\n",
    "\r\n",
    "<h3 id=\"entropy\">Entropy</h3>  \r\n",
    "\r\n",
    "  - Entropy $x \\cdot \\ln x$ is strictly convex function.\r\n",
    "  ```r\r\n",
    "  x = seq(0,1, length = 10000)\r\n",
    "  x = x[x>0]\r\n",
    "  plot(x=x, y = x*log(x), col=2)\r\n",
    "  ```\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}